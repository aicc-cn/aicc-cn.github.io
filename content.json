{"meta":{"title":"AICC","subtitle":"Artificial Intelligence Chinese Community","description":"Artificial Intelligence Chinese Community","author":"AICC","url":"https://aicc-cn.github.io","root":"/"},"pages":[{"title":"about","date":"2022-03-25T13:11:08.000Z","updated":"2022-03-25T13:11:08.443Z","comments":true,"path":"about/index.html","permalink":"https://aicc-cn.github.io/about/index.html","excerpt":"","text":""},{"title":"machine","date":"2022-03-25T12:50:14.000Z","updated":"2022-03-25T12:50:14.759Z","comments":true,"path":"machine/index.html","permalink":"https://aicc-cn.github.io/machine/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2022-03-25T13:35:42.421Z","updated":"2022-03-25T13:35:42.421Z","comments":true,"path":"tags/index.html","permalink":"https://aicc-cn.github.io/tags/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2022-03-25T13:05:20.764Z","updated":"2022-03-25T13:05:20.764Z","comments":true,"path":"categories/index.html","permalink":"https://aicc-cn.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"【十四】光流估计","slug":"2019-03-14-cv-image_LK","date":"2019-03-22T16:00:00.000Z","updated":"2022-03-25T13:52:01.871Z","comments":true,"path":"2019/03/23/2019-03-14-cv-image_LK/","link":"","permalink":"https://aicc-cn.github.io/2019/03/23/2019-03-14-cv-image_LK/","excerpt":"","text":"光流估计【简介】 ​ 在计算机视觉中，Lucas–Kanade光流算法是一种两帧差分的光流估计算法。它由Bruce D. Lucas 和 Takeo Kanade提出。 【光流的概念】 (Optical flow or optic flow)它是一种运动模式，这种运动模式指的是一个物体、表面、边缘在一个视角下由一个观察者（比如眼睛、摄像头等）和背景之间形成的明显移动。 ​ 光流技术，如运动检测和图像分割，时间碰撞，运动补偿编码，三维立体视差，都是利用了这种边缘或表面运动的技术。 ​ 二维图像的移动相对于观察者而言是三维物体移动的在图像平面的投影。 ​ 有序的图像可以估计出二维图像的瞬时图像速率或离散图像转移。 【光流算法】 ​ 它评估了两幅图像的之间的变形，它的基本假设是体素和图像像素守恒。它假设一个物体的颜色在前后两帧没有巨大而明显的变化。 ​ 基于这个思路，我们可以得到图像约束方程。不同的光流算法解决了假定了不同附加条件的光流问题。 【基本模型】 基本思想：虽然$t+1$的时候位置进行了移动，如果我们知道灰度是连续变化的，那么我们就可以通过像素的变化推演当前这个中心点大概移动了多少。 通过当前位置亮度在$I_x，I_y$以及相邻两帧中灰度的变化值$I_t$，来推演$\\delta _x，\\delta_y$. 我们在检测物体上建立一个小方格里的所有像素位移相同，建立矩阵方程组，解出u、v两个未知数。 最优化问题（超定方程求解） $min |Au-b|$ 最小二乘解 $u&#x3D;(A^TA)^{-1}A^Tb$ 区域像素只有2个时，就是2元1次方程组求解！ 多个像素，比如3∗3时，则是求上述最小二乘解. 【Lucas–Kanade方法】 ​ 这个算法是最常见，最流行的。它计算两帧在时间t 到t + δt之间每个每个像素点位置的移动。 由于它是基于图像信号的泰勒级数，这种方法称为差分，这就是对于空间和时间坐标使用偏导数。 可信度判断：矩阵求逆是否能实现？ 该表达式是和沿着x和y的梯度是密切相关的，只要其中一个为0时，$A^TA$就不可逆。只要当该区域的像素点比较平滑，矩阵就可逆 通过求特征值来判断计算可信。 两个特征值都远大于0，那么就可逆，否则就不可逆或者很难求逆。 【金字塔L-K方法】 针对目标运动很快的情况，用原始的L-K方法就没有办法处理了， 需要采用图像金字塔的方法。把图像从小到大的顺序排列。 ​ 金字塔特征跟踪算法描述如下：首先，光流和仿射变换矩阵在最高一层的图像上计算出；将上一层的计算结果作为初始值传递给下一层图像，这一层的图像在这个初始值的基础上，计算这一层的光流和仿射变化矩阵；再将这一层的光流和仿射矩阵作为初始值传递给下一层图像，直到传递给最后一层，即原始图像层，这一层计算出来的光流和仿射变换矩阵作为最后的光流和仿射变换矩阵的结果。 用不同像素的图片建立金字塔 运行L-K方法 优点：可处理大位移；对噪声更不敏感。 【数学模型】 总结 光流估计基于恒定亮度假设模型 L-K光流估计方法利用了邻域内的运动不变性 金字塔L-K方法可有效提高光流计算对大位移的鲁棒性 代码实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/1/22 22:20# @Author : Seven# @File : sightDemo.py# @Software: PyCharmimport cv2import numpy as np# 加载视频cap = cv2.VideoCapture()cap.open(&#x27;768x576.avi&#x27;)if not cap.isOpened(): print(&quot;无法打开视频文件&quot;)lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))feature_params = dict(maxCorners=500, qualityLevel=0.3, minDistance=7, blockSize=7)class App: def __init__(self, video_src): # 构造方法，初始化一些参数和视频路径 self.track_len = 10 self.detect_interval = 5 self.tracks = [] self.cam = cv2.VideoCapture(video_src) self.frame_idx = 0 def run(self): # 光流运行方法 while True: ret, frame = self.cam.read() # 读取视频帧 if ret: frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # 转化为灰度虚图像 vis = frame.copy() if len(self.tracks) &gt; 0: # 检测到角点后进行光流跟踪 img0, img1 = self.prev_gray, frame_gray p0 = np.float32([tr[-1] for tr in self.tracks]).reshape(-1, 1, 2) p1, st, err = cv2.calcOpticalFlowPyrLK(img0, img1, p0, None, **lk_params) # 前一帧的角点和当前帧的图像作为输入来得到角点在当前帧的位置 p0r, st, err = cv2.calcOpticalFlowPyrLK(img1, img0, p1, None, **lk_params) # 当前帧跟踪到的角点及图像和前一帧的图像作为输入来找到前一帧的角点位置 d = abs(p0 - p0r).reshape(-1, 2).max(-1) # 得到角点回溯与前一帧实际角点的位置变化关系 good = d &lt; 1 # 判断d内的值是否小于1，大于1跟踪被认为是错误的跟踪点 new_tracks = [] for tr, (x, y), good_flag in zip(self.tracks, p1.reshape(-1, 2), good): # 将跟踪正确的点列入成功跟踪点 if not good_flag: continue tr.append((x, y)) if len(tr) &gt; self.track_len: del tr[0] new_tracks.append(tr) cv2.circle(vis, (x, y), 2, (0, 255, 0), -1) self.tracks = new_tracks cv2.polylines(vis, [np.int32(tr) for tr in self.tracks], False, (0, 255, 0)) # 以上一振角点为初始点，当前帧跟踪到的点为终点划线 # draw_str(vis, (20, 20), &#x27;track count: %d&#x27; % len(self.tracks)) if self.frame_idx % self.detect_interval == 0: # 每5帧检测一次特征点 mask = np.zeros_like(frame_gray) # 初始化和视频大小相同的图像 mask[:] = 255 # 将mask赋值255也就是算全部图像的角点 for x, y in [np.int32(tr[-1]) for tr in self.tracks]: # 跟踪的角点画圆 cv2.circle(mask, (x, y), 5, 0, -1) p = cv2.goodFeaturesToTrack(frame_gray, mask=mask, **feature_params) # 像素级别角点检测 if p is not None: for x, y in np.float32(p).reshape(-1, 2): self.tracks.append([(x, y)]) # 将检测到的角点放在待跟踪序列中 self.frame_idx += 1 self.prev_gray = frame_gray cv2.imshow(&#x27;lk_track&#x27;, vis) ch = 0xFF &amp; cv2.waitKey(100) if ch == 27: breakApp(&#x27;768x576.avi&#x27;).run()","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【十六】相对位姿测量算法","slug":"2019-03-16-cv-image_position","date":"2019-03-22T16:00:00.000Z","updated":"2022-03-25T13:52:13.975Z","comments":true,"path":"2019/03/23/2019-03-16-cv-image_position/","link":"","permalink":"https://aicc-cn.github.io/2019/03/23/2019-03-16-cv-image_position/","excerpt":"","text":"相对位姿测量算法【基于空间多点】 相对位姿估计的基本问题 已知：相机内参数；多个空间上的特征点(非共面)在目标坐标系(3D)和相平面坐标系(2D)坐标。 输出：目标坐标系相对相机坐标系的位置和姿态。 基本思想示意 线性求解 对每一个特征点，均有： 对每一个特征点，均有： 展开第一行 类似展开第二、第三行： 消去$Z_c$ 上二式右侧分母移到左边，得： 整理为矩阵形式 对于每一个点都可以形成如上两个方程，对于多个点，可进行堆叠，并记成矩阵形式： 有六个或以上特征点且非共面时，可求解： 上面求出的只有11个参数，且多一个&#x2F;t3. 最后一个变量可利用如下约束求出 线性求解总结 利用矩阵的QR分解，得到最终的旋转矩阵非奇异矩阵P的正交三角分解：P&#x3D;QR, 其中Q(维数n*s): 正交阵； R :上三角阵 证明思路：对P 中各向量进行正交化 【扩展】 根据旋转矩阵计算旋转角 相机坐标系想要转到与世界坐标系完全平行（即坐标轴完全平行，且方向相同），需要旋转3次，设原始相机坐标系为C01、 C0绕其Z轴旋转，得到新的坐标系C1；2、 C1绕其Y轴旋转，得到新的坐标系C2（注意旋转轴为C1的Y轴，而非C0的Y轴）；3、 C2绕其X轴旋转，得到新的坐标系C3。此时C3与世界坐标系完全平行。 Rodrigues旋转 空间的任何一个旋转，可表达为一个向量绕旋转轴旋转给定角度。可用四元数表达： 【基于平面多特征点】 基本问题 已知：相机内参数；多个平面上的特征点在目标坐标系(3D)和相平面坐标系(2D)坐标。 输出：目标坐标系相对相机坐标系的位置和姿态。 平面特征点相对位姿估计——线性求解 设$Z_t&#x3D;0$(特征共面), 则对每一个特征点，均有： 得到两个方程 未知数线性求解 对于每一个点都可以形成如上两个方程，对于&gt;&#x3D;4个点，可使用类似PnP方法求得解： 【总结】 在已知至少六个空间点三维点坐标的条件下，可通过点的图像坐标及相对位姿估计算法计算相对位姿。 在已知至少四个平面点三维点坐标的条件下，可通过点的图像坐标及相对位姿估计算法计算相对位姿。 代码演示1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/2/26 21:25# @Author : Seven# @File : PositionMeasurement.py# @Software: PyCharm# function : 实现位姿测量算法import cv2import numpy as npimport glob# 加载相机标定的数据with np.load(&#x27;C.npz&#x27;) as X: mtx, dist, _, _ = [X[i] for i in (&#x27;mtx&#x27;, &#x27;dist&#x27;, &#x27;rvecs&#x27;, &#x27;tvecs&#x27;)]def draw(img, corners, imgpts): &quot;&quot;&quot; 在图片上画出三维坐标轴 :param img: 图片原数据 :param corners: 图像平面点坐标点 :param imgpts: 三维点投影到二维图像平面上的坐标 :return: &quot;&quot;&quot; corner = tuple(corners[0].ravel()) cv2.line(img, corner, tuple(imgpts[0].ravel()), (255, 0, 0), 5) cv2.line(img, corner, tuple(imgpts[1].ravel()), (0, 255, 0), 5) cv2.line(img, corner, tuple(imgpts[2].ravel()), (0, 0, 255), 5) return img# 初始化目标坐标系的3D点objp = np.zeros((6 * 7, 3), np.float32)objp[:, :2] = np.mgrid[0:7, 0:6].T.reshape(-1, 2)# 初始化三维坐标系axis = np.float32([[3, 0, 0], [0, 3, 0], [0, 0, -3]]).reshape(-1, 3) # 坐标轴# 加载打包所有图片数据images = glob.glob(&#x27;image/*.jpg&#x27;)for fname in images: img = cv2.imread(fname) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # 找到图像平面点坐标点 ret, corners = cv2.findChessboardCorners(gray, (7, 6), None) if ret: # PnP计算得出旋转向量和平移向量 _, rvecs, tvecs, _ = cv2.solvePnPRansac(objp, corners, mtx, dist) print(&quot;旋转变量&quot;, rvecs) print(&quot;平移变量&quot;, tvecs) # 计算三维点投影到二维图像平面上的坐标 imgpts, jac = cv2.projectPoints(axis, rvecs, tvecs, mtx, dist) # 把坐标显示图片上 img = draw(img, corners, imgpts) cv2.imshow(&#x27;img&#x27;, img) cv2.waitKey(500)cv2.destroyAllWindows() 运行结果","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【十七】相机标定","slug":"2019-03-17-cv-image_calibration","date":"2019-03-22T16:00:00.000Z","updated":"2022-03-25T13:52:22.104Z","comments":true,"path":"2019/03/23/2019-03-17-cv-image_calibration/","link":"","permalink":"https://aicc-cn.github.io/2019/03/23/2019-03-17-cv-image_calibration/","excerpt":"","text":"相机标定【基本问题】 相机内外参数标定 计算$m_{ij}$的解 分解内、外参数 考虑非线性项 【Zhang方法】 Zhang方法：由张正友提出，OpenCV等广泛使用 特点：使用平面靶标摆多个pose(可未知) 标定步骤 对一个pose,计算单应矩阵(类似M矩阵) 有三个以上Pose，根据各单应矩阵计算线性相机参数； 使用非线性优化方法计算非线性参数 第一步：求解单应矩阵——基本方程 特点：使用平面靶标摆多个pose(可未知) 平面靶标有四个点或更多时，可求解H(差一比例因子) 第二步：求解单应矩阵——建立内参数方程 根据R约束 对应每一个pose,可得到上述两个方程 第三步：求解内参数——建立方程 令 $B&#x3D;(b_{ij})&#x3D;M_1^{-T}M_1^{-1}$ 根据B对称，定义参数向量 第四步：求解参数——建立内参数方程 第五步：求解参数——内参数求解 当n&gt;&#x3D;3 时,可求解b 解为： 前式代入，可知： 进一步可确定$M_1$各参数 第六步：求解参数——外参数求解 系数 最后一步：非线性畸变参数求解 以已有解为初值，求解下式： 可使用Levenberg-Marquardt算法求解(原方法中只含径向畸变) 外参数标定结果示意 重投影示意 【总结】 Zhang方法从多个角度拍摄平面标定物，进一步通过特征点计算内、外参数及非线性畸变 与已有方法相比，Zhang方法简单，不需要已知目标特征点三维坐标，因而得到广泛应用 注意：参与标定的数据一般为30张左右。 代码演示123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/2/26 20:15# @Author : Seven# @File : CameraCalibration.py# @Software: PyCharm# function : 摄像机标定import cv2import numpy as npimport glob# 设置寻找亚像素角点的参数，采用的停止准则是最大循环次数30和最大误差容限0.001criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)# 准备目标点，例如 (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)objp = np.zeros((6 * 7, 3), np.float32)# 将世界坐标系建在标定板上，所有点的Z坐标全部为0，所以只需要赋值x和yobjp[:, :2] = np.mgrid[0:7, 0:6].T.reshape(-1, 2)# 用于存储所有图像中的对象点和图像点的数组。objpoints = [] # 存储在现实世界空间的3d点imgpoints = [] # 储存图像平面中的2d点。images = glob.glob(&#x27;image/*.jpg&#x27;)for fname in images: img = cv2.imread(fname) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # 获取XY坐标 size = gray.shape[::-1] # 找到棋盘角点 ret, corners = cv2.findChessboardCorners(gray, (7, 6), None) # 如果找到，添加3D点，2D点 if ret: objpoints.append(objp) # 增加角点的准确度 corners2 = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria) imgpoints.append(corners2) # 画出并显示角点 img = cv2.drawChessboardCorners(img, (7, 6), corners2, ret) cv2.imshow(&#x27;img&#x27;, img) cv2.waitKey(500)# 相机标定ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, size, None, None)# 保存相机参数np.savez(&#x27;C.npz&#x27;, mtx=mtx, dist=dist, rvecs=rvecs, tvecs=tvecs)print(&quot;ret:&quot;, ret)print(&quot;内参数矩阵:\\n&quot;, mtx)print(&quot;畸变系数:\\n&quot;, dist)print(&quot;旋转向量:&quot;, rvecs) # 外参数print(&quot;平移向量:&quot;, tvecs) # 外参数 运行结果：","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【十五】坐标变换与摄像机模型","slug":"2019-03-15-cv-image_camera","date":"2019-03-22T16:00:00.000Z","updated":"2022-03-25T13:52:08.285Z","comments":true,"path":"2019/03/23/2019-03-15-cv-image_camera/","link":"","permalink":"https://aicc-cn.github.io/2019/03/23/2019-03-15-cv-image_camera/","excerpt":"","text":"坐标变换与摄像机模型图像变换模型【平移变换】 只改变图形位置，不改变图形的大小和形状. 代码及演示 123456789101112131415import cv2import numpy as np# 加载图片source_image = cv2.imread(&#x27;lena.jpg&#x27;, 0)cv2.imshow(&#x27;source&#x27;, source_image)# 平移图片rows, cols = source_image.shape# 创建交换矩阵，按（100,50）平移M = np.float32([[1, 0, 100], [0, 1, 50]])# 使用仿射变换的api进行平移变换translation_image = cv2.warpAffine(source_image, M, (cols, rows))cv2.imshow(&#x27;translation&#x27;, translation_image)cv2.waitKey(0) 【旋转变换】 将输入图像绕笛卡尔坐标系的原点逆时针旋转$\\theta$角度， 则变换后图像坐标为 代码及演示 123456789101112131415import cv2import numpy as np# 加载图片source_image = cv2.imread(&#x27;lena.jpg&#x27;, 0)cv2.imshow(&#x27;source&#x27;, source_image)# 获取形状rows, cols = source_image.shape# 旋转图片# 创建交换矩阵， 旋转90度M = cv2.getRotationMatrix2D(((cols-1)/2., (rows-1)/2.), 90, 1)# 使用仿射变换的api进行旋转变换rotation_image = cv2.warpAffine(source_image, M, (cols, rows))cv2.imshow(&#x27;rotation&#x27;, rotation_image)cv2.waitKey(0) 【比例变换】 若图像坐标$(x, y)$ 缩放到$(S_x, S_y)$倍，则变换函数为： 其中$S_y$, 分别为 x和y 坐标的缩放因子，其大于1表示放大，小于1表示缩小。 代码及演示 1234567891011121314import cv2import numpy as np# 加载图片source_image = cv2.imread(&#x27;lena.jpg&#x27;, 0)cv2.imshow(&#x27;source&#x27;, source_image)# 获取形状rows, cols = source_image.shape# 比例变换# 缩小一半proportion_image = cv2.resize(source_image, (cols//2, rows//2), cv2.INTER_CUBIC)cv2.imshow(&#x27;proportion&#x27;, proportion_image)cv2.waitKey(0) 【仿射变换】 简单的来说就是一个线性变换加上平移 仿射变换的一般表达式为: 平移、比例缩放和旋转变换都是一种称为仿射变换的特殊情况。 仿射变换的性质 仿射变换有6个自由度（对应变换中的6个系数），因此，仿射变换后互相平行直线仍然为平行直线，三角形映射后仍是三角形。但却不能保证将四边形以上的多边形映射为等边数的多边形。 仿射变换，可以保持原来的线共点，点共线的关系不变，保持原来相互平行的的先仍然相互平行，保持原来在一直线上几段线段之间的比例 关系不变。但是，仿射变换不能保持原来的线段长度不变，也不能保持原来的夹角角度不变。 仿射变换的乘积和逆变换仍是仿射变换。 仿射变换能够实现平移、旋转、缩放等几何变换。 代码及演示 1234567891011121314151617181920212223import cv2import numpy as npimport matplotlib.pyplot as plt# 加载图片source_image = cv2.imread(&#x27;lena.jpg&#x27;, 0)cv2.imshow(&#x27;source&#x27;, source_image)# 获取形状rows, cols = source_image.shape# 仿射变换# 定义交换矩阵pts1 = np.float32([[50, 50], [200, 50], [50, 200]])pts2 = np.float32([[10, 100], [200, 50], [100, 250]])M = cv2.getAffineTransform(pts1, pts2)# 开始变换affine_image = cv2.warpAffine(source_image, M, (cols, rows))plt.subplot(121), plt.imshow(source_image), plt.title(&#x27;Input&#x27;)plt.subplot(122), plt.imshow(affine_image), plt.title(&#x27;Output&#x27;)plt.show()cv2.imshow(&#x27;affine&#x27;, affine_image)cv2.waitKey(0) 【透视变换】 把物体的三维图像表示转变为二维表示的过程，称为透视变换，也称为投影映射，其表达式为: 透视变换也是一种平面映射 ，并且可以保证任意方向上的直线经过透视变换后仍然保持是直线。 透视变换具有9个自由度（其变换系数为9个），故可以实现平面四边形到四边形的映射。 透视变化示意 对于透视投影，一束平行于投影面的平行线的投影可保持平行，而不平行于投影面的平行线的投影会聚集到一个点，该点称灭点. 代码及演示 1234567891011121314151617181920212223import cv2import numpy as npimport matplotlib.pyplot as plt# 加载图片source_image = cv2.imread(&#x27;lena.jpg&#x27;, 0)cv2.imshow(&#x27;source&#x27;, source_image)# 获取形状rows, cols = source_image.shape# 透视变换# 定义交换矩阵pts1 = np.float32([[56, 65], [368, 52], [28, 387], [389, 390]])pts2 = np.float32([[0, 0], [300, 0], [0, 300], [300, 300]])M = cv2.getPerspectiveTransform(pts1, pts2)# 开始变换perspective_image = cv2.warpPerspective(source_image, M, (300, 300))plt.subplot(121), plt.imshow(source_image), plt.title(&#x27;Input&#x27;)plt.subplot(122), plt.imshow(perspective_image), plt.title(&#x27;Output&#x27;)plt.show()cv2.imshow(&#x27;perspective&#x27;, proportion_image)cv2.waitKey(0) 坐标系和坐标变换 不同坐标系及坐标变换关系 当物体旋转时，其上的点在固定坐标系坐标值变化。 任意两个三维坐标系之间的变换关系 注意：R满足旋转矩阵正交性约束 坐标系变换及旋转矩阵生成示意图 【像素坐标系】 像素坐标系uov是一个二维直角坐标系，反映了相机CCD&#x2F;CMOS芯片中像素的排列情况。原点o位于图像的左上角，u轴、v轴分别于像面的两边平行。像素坐标系中坐标轴的单位是像素（整数）。 【图像坐标系】 图像坐标系(x,y)：像素坐标系不利于坐标变换，因此需要建立图像坐标系XOY，其坐标轴的单位通常为毫米（mm），原点是相机光轴与相面的交点（称为主点），即图像的中心点，X轴、Y轴分别与u轴、v轴平行。故两个坐标系实际是平移关系，即可以通过平移就可得到。 【摄像机坐标系】 相机坐标系（camera coordinate）($O_cX_cY_cZ_c (camera frame)$)，也是一个三维直角坐标系，原点位于镜头光心处，x、y轴分别与相面的两边平行，z轴为镜头光轴，与像平面垂直。 【世界坐标系】 世界坐标系（world coordinate）($O_wX_wY_wZ_w$)，也称为测量坐标系，是一个三维直角坐标系，以其为基准可以描述相机和待测物体的空间位置。世界坐标系的位置可以根据实际情况自由确定。 【手端坐标系或平台坐标系】 $O_eX_eY_eZ_e$ 【目标坐标系】 $O_tX_tY_tZ_t$ 【坐标系转换】 世界坐标系转换为相机坐标系 其中R为3*3的旋转矩阵，t为3*1的平移矢量，即相机外参数 相机坐标系转换为图像坐标系 s为比例因子（s不为0），f为有效焦距（光心到图像平面的距离），(x,y,z,1)是空间点P在相机坐标系oxyz中的齐次坐标，(X,Y,1)是像点p在图像坐标系OXY中的齐次坐标。 图像坐标系转换为像素坐标系 其中，dX、dY分别为像素在X、Y轴方向上的物理尺寸，$u_0,v_0$为主点（图像原点）坐标 世界坐标系转换为像素坐标系 其中，$m_1、m_2$即为相机的内参和外参数。 注意：R,t参数矩阵为不可逆矩阵。 线性及非线性摄像机模型【线性摄像机模型】 考虑简化的针孔模型 ​ 针孔模型是各种相机模型中最简单的一种，它是相机的一个近似线性模型。在相机坐标系下，任一点$P(X_c,Y_c,Z_c)$在像平面的投影位置,也就是说，任一点$P(X_c,Y_c,Z_c)$的投影点p(x,y)都是OP（即光心（投影中心）与点$P(X_c,Y_c,Z_c)$的连线）与像平面的交点如下图。 加入相机坐标系与世界坐标系变换关系，得到 说明：上述公式中完成了从世界坐标系到图像坐标系的转变，中间经过了相机坐标系的过度，$X_w$中的w表示world世界，单位为毫米，而u,v是的 单位为像素，即完成了从毫米——像素的转换。 所以，为了得到空间物体的三维世界坐标，就必须有两个或更多的相机构成立体视觉系统模型才能实现。 成像畸变示意 【非线性摄像机模型】 在实际的成像过程中，考虑镜头的失真，一般都存在非线性畸变，所以线性模型不能准确描述成像几何关系。非线性畸可用下列公式描述： 径向畸变，离心畸变，薄棱镜畸变 通常只考虑径向畸变： 若考虑非线性畸变，则对相机标定时需要使用非线性优化算法。而有研究表明引入过多的非线性参入（如离心畸变和薄棱畸变）不仅不能提高精度，还会引起解的不稳定。一般情况下径向畸变就足以描述非线性畸变，所有本课题只是考虑径向畸变。则将式(2.9)中的径向畸变代入式(2.8)可得： 总结 图像变换包括平秱、旋转、仿射、透视变换等 常用的坐标系包括像素坐标系、相机坐标系、世界坐标系等，任一点在世界坐标系和相机坐标系中的坐标通过投影矩阵M相关联，M包含了内参数和外参数矩阵 一般相机成像存在非线性畸变，重点需要考虑径向畸变","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【十九】立体视觉","slug":"2019-03-19-cv-image_Stereo","date":"2019-03-22T16:00:00.000Z","updated":"2022-03-25T13:52:33.595Z","comments":true,"path":"2019/03/23/2019-03-19-cv-image_Stereo/","link":"","permalink":"https://aicc-cn.github.io/2019/03/23/2019-03-19-cv-image_Stereo/","excerpt":"","text":"立体视觉【基本概念】 我们在二维视角中，结构和深度是不确定的 立体视觉：第2个照相机可以解决这种歧义性，通过三角化实现深度测量 【平行双目视觉】 假设双目完全平行 空间点三维座标位置求解 视差和深度成反比关系: $z_1 &#x3D; \\frac{bf_x}{u_1-u_2}$ 空间点三维座标位置求解：一般情况 二摄像机坐标系与世界坐标系位姿关系已知 共4个方程，三个未知数，可求解坐标 【三维重构】 三维重构步骤 提取特征点，建立特征匹配 计算视差 计算世界坐标 三角剖分 三维重构 提取特征点并建立匹配 特征匹配方式 特征点提取+特征匹配 光流匹配 块匹配 立体矫正+平行匹配 视差计算 计算世界坐标–形成点云数据 三角剖分：采用经典的Delauney算法 Delauney算法示意： Delaunay三角网是唯一的（任意四点不能共圆），在Delaunay三角形网中任一三角形的外接圆范围内不会有其它点存在。 三维重构：基于计算坐标，采用OpenGL绘制三角片 效果不好的主要原因，是图像中深度变化较大，同时灰度变换，而特征点选取的比较稀疏. 【总结】 立体视觉可计算空间点的三维坐标。基线越长，距离越近，精度越高 根据双目视觉进行三维重构包括特征点提取、匹配、坐标计算、三角剖分、三维重构等几个步骤","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【十八】极线几何","slug":"2019-03-18-cv-image_PolarGeometry","date":"2019-03-22T16:00:00.000Z","updated":"2022-03-25T13:52:28.553Z","comments":true,"path":"2019/03/23/2019-03-18-cv-image_PolarGeometry/","link":"","permalink":"https://aicc-cn.github.io/2019/03/23/2019-03-18-cv-image_PolarGeometry/","excerpt":"","text":"极线几何 有时候我们使用一个相机进行拍摄目标物体的时候，会 发现几个物体都重合了，但是当我们再放台相机的时候，就可以把这些物体的特征获取到。 也就是双目视觉对应关系，同时也可用于相邻两帧间的运动估计。 【基本概念】 极线几何(Epipolar geometry) (匹配)极线约束：匹配点必须在极线上 基线：左右像机光心连线 极平面：空间点，两像机光心决定的平面 极点：基线与两摄像机图像平面的交点 极线：极平面与图片平面的交线 【本质矩阵】 本质矩阵 E（Essential Matrix）：反映【空间一点 P 的像点】在【不同视角摄像机】下【摄像机坐标系】中的表示之间的关系。 前面我们已经知道了各个坐标系之前的转换 相机坐标系与世界坐标系 相机坐标系与图像坐标系 两相机坐标系某点与对应图像坐标系的关系： 同一点在两相机坐标系之间的关系： 两边同时叉积$t$： 再与$p_r^\\sim​$点积： 【本质矩阵求解】 基本方程 线性方程求解 有九个点(非共面)时，可获得线性解： 注意：解与真实解相差一个比例系数 使用SVD分解求解平移和旋转矩阵 可以证明，本质矩阵有2个相同的非零特征值 因此，最终可以得到4个解，但仅有一个合理解 【扩展】 基本矩阵(Fundamental matrix)： 反映【空间一点 P 的像素点】在【不同视角摄像机】下【图像坐标系】中的表示之间的关系。 两个对应点在像素座标系的对应关系(包含相机内参数信息) 【总结】 极线是极平面和像平面交线，极点是极线和基线交点 本质矩阵确定了两帧图像中对应点的约束关系 可以通过8个对应点求解本质矩阵，进一步分解得到R和t 代码实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/3/1 22:03# @Author : Seven# @File : PolarGeometry.py# @Software: PyCharm# function : 极线几何，测量极线import cv2import numpy as npimport matplotlib.pyplot as plt# 加载图片img1 = cv2.imread(&#x27;image/l.jpg&#x27;, 0)img2 = cv2.imread(&#x27;image/r.jpg&#x27;, 0)# 初始化SIFT方法sift = cv2.xfeatures2d_SIFT.create()# 获取关键点和描述子k1, d1 = sift.detectAndCompute(img1, None)k2, d2 = sift.detectAndCompute(img2, None)# 设置FLANN 超参数FLANN_INDEX_KDTREE = 0# K-D树索引超参数index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)# 搜索超参数search_params = dict(checks=50)# 初始化FlannBasedMatcher匹配器flann = cv2.FlannBasedMatcher(index_params, search_params)# 通过KNN的方式匹配两张图的描述子matches = flann.knnMatch(d1, d2, k=2)good = []pts1 = []pts2 = []# 筛选比较好的匹配点for i, (m, n) in enumerate(matches): if m.distance &lt; 0.8 * n.distance: good.append(m) pts2.append(k2[m.trainIdx].pt) pts1.append(k1[m.queryIdx].pt)# 计算基础矩阵pts1 = np.int32(pts1)pts2 = np.int32(pts2)# F为基本矩阵、mask是返回基本矩阵的值：没有找到矩阵，返回0，找到一个矩阵返回1，多个矩阵返回3F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.FM_LMEDS)# 只选择有效数据pts1 = pts1[mask.ravel() == 1]pts2 = pts2[mask.ravel() == 1]def drawlines(img1, img2, lines, pts1, pts2): &quot;&quot;&quot; 绘制图像极线 :param img1: :param img2: :param lines: :param pts1: :param pts2: :return: &quot;&quot;&quot; r, c = img1.shape img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR) img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR) for r, pt1, pt2 in zip(lines, pts1, pts2): color = tuple(np.random.randint(0, 255, 3).tolist()) x0, y0 = map(int, [0, -r[2] / r[1]]) x1, y1 = map(int, [c, -(r[2] + r[0] * c) / r[1]]) img1 = cv2.line(img1, (x0, y0), (x1, y1), color, 1) img1 = cv2.circle(img1, tuple(pt1), 5, color, -1) img2 = cv2.circle(img2, tuple(pt2), 5, color, -1) return img1, img2# 在右图（第二图）中找到与点相对应的极线，并在左图上画出它的线。lines1 = cv2.computeCorrespondEpilines(pts2.reshape(-1, 1, 2), 2, F)lines1 = lines1.reshape(-1, 3)img5, img6 = drawlines(img1, img2, lines1, pts1, pts2)# 找到与左图像（第一个图像）中的点对应的极线，以及在右图上画线lines2 = cv2.computeCorrespondEpilines(pts1.reshape(-1, 1, 2), 1, F)lines2 = lines2.reshape(-1, 3)img3, img4 = drawlines(img2, img1, lines2, pts2, pts1)plt.subplot(121), plt.imshow(img5)plt.subplot(122), plt.imshow(img3)plt.show()","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【二十】特征匹配","slug":"2019-03-20-cv-image_feature","date":"2019-03-22T16:00:00.000Z","updated":"2022-03-25T13:52:38.780Z","comments":true,"path":"2019/03/23/2019-03-20-cv-image_feature/","link":"","permalink":"https://aicc-cn.github.io/2019/03/23/2019-03-20-cv-image_feature/","excerpt":"","text":"特征匹配【基本问题】 特征点匹配 暴力搜索与2NN判据 设两帧图像中的对应特征点集${𝑥𝑖,𝑦_𝑖}$和${𝑥_𝑖^′,𝑦_𝑖^′}$，共N个特征点 对应点 $𝐱_𝒋&#x3D;(𝑥_𝑗,𝑦_𝑗)$，匹配点为距离最小点 $𝐱_𝑗^′&#x3D;𝑥_𝑗^′∗,𝑦_𝑗^′∗&#x3D;𝑚𝑖𝑛{𝑘&#x3D;1}^𝑁||x_𝑗−𝑥_𝑘^′||$ ，对应距离$𝑑_𝑗^∗$ 进一步得到次小点𝐱′′，对应距离$𝑑_𝑗^{∗′}$, 则匹配点满足：$𝑑_𝑗^∗&lt;𝛼∗𝑑_𝑗^{∗′}$, 认为正常匹配. 【特征点匹配方法】 快速搜索方式—-二叉树 算法复杂度o(N2)。特征点数量多时，匹配效率低。 二叉搜索树(BST)提供了高效搜索方式 以下是一颗一维的二叉搜索树，尝试搜索和11最近的点。 K-D树 对于每一层，可以指定一个划分维度（轴垂直分区面axis-aligned splitting planes）。最简单的就是按照关键字轮流划分（例如：奇数层按照x轴划分，也即第一个关键字；偶数层按照y轴划分，也即第二个关键字）。 K-D树的建立方式 对于所有的样本点，统计它们在每个维上的方差，挑选出方差中的最大值，对应的维就是分裂域的值。数据方差最大表明沿该维度数据点分散得比较开，这个方向上进行数据分割可以获得最好的分辨率；然后再将所有样本点按其第该维的值迕行排序，位于正中间的那个数据点选为分裂结点对应域。重复上述过程直至获得所有叶子节点显示了构建返棵二叉树的所有步骤。 下面以一个简单的例子来解释上述k-d tree的构建过程。 假设样本集为：{(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)}。 K-D树最近邻查询算法 首先通过将查找点数据根结点数据对应维 上的值相比较，按照二叉搜索的方式，顺着“搜索路径”找到最近邻的近似点，也就是 与查询点处于同一个子空间的叶子节点； 为了防止漏查与查找点 跟进的距离的点，回溯搜索路径，并且判断搜索路径上节点的其他子节点空 间中是否还有距离查询点更近的数据点，如果有，则需要跳到其他子节点空间中去搜索。 重复返个过程直到搜索路径为空。 BBF(Best Bin First) ​ BBF的查询思路就是将“查询路径”上的节点进行排序，如按各自分割超平面（称为Bin）与查询点的距离排序,优先考虑距离小的点。BBF还设置了一个运行超时限制，当优先级队列中的所有节点都经过检查或者超出时间限制时，算法返回当前找到的最好结果作为近似的最近邻。 随机化K-D森林 同时独立建立多个k-d树，每棵树在具有大方差的各维中(如top-5)随机选择。查询时，并行查询多个k-d树，按照BBF准侧将候选节点放在同一队列中。 【RANSAC】 稳健(robust): 对数据噪声的敏感性 基本思想 RANSAC通过反复选择数据中的一组随机子集来达成目标。被选取的子集被假设为局内点，并用下述方法进行验证： 有一个模型适应于假设的局内点，即所有的未知参数都能从假设的局内点计算得出。 用1中得到的模型去测试所有的其它数据，如果某个点适用于估计的模型，认为它也是局内点。 如果有足够多的点被归类为假设的局内点，那么估计的模型就足够合理。 然后，用所有假设的局内点去重新估计模型，因为它仅仅被初始的假设局内点估计过。 最后，通过估计局内点与模型的错误率来评估模型。 这个过程被重复执行固定的次数，每次产生的模型要么因为局内点太少而被舍弃，要么因为比现有的模型更好而被选用 SIFT和RANSAC结合 RANSAC算法在SIFT特征筛选中的主要流程： 从样本集中随机抽选一个RANSAC样本，即4个匹配点对 根据返4个匹配点对计算变换矩阵M 根据样本集，变换矩阵M，和误差度量函数计算满足当前变换矩阵的一致集consensus，并返回一致集中元素个数 根据当前一致集中元素个数判断是否最优(最大)一致集，若是则更新当前最优一致集 更新当前错误概率p，若p大于允许的最小错误概率则重复(1)至(4)继续迭代，直到当前错误概率p小于最小错误概率 结果比较：2NN vs 2NN+RANSAC 【总结】 特征匹配是立体视觉、全景视觉、由运动到结构的重要环节 使用基于k-d树的特征匹配方法，能有效提高搜索效率 RANSAC是一类随机稳健估计方法，可有效滤除误配点","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"基于Harris的特征匹配","slug":"2019-03-23-cv-Harris_feature","date":"2019-03-22T16:00:00.000Z","updated":"2022-03-25T13:38:22.216Z","comments":true,"path":"2019/03/23/2019-03-23-cv-Harris_feature/","link":"","permalink":"https://aicc-cn.github.io/2019/03/23/2019-03-23-cv-Harris_feature/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/3/4 14:35# @Author : Seven# @File : ImageMatching-Harris.py# @Software: PyCharm# function : 使用Harris进行提取特征点进行图片匹配# 选择了一种速度、特征点数量和精度都比较好的组合方案：# FAST角点检测算法+SURF特征描述子+FLANN(Fast Library for Approximate Nearest Neighbors) 匹配算法。import cv2# 加载图片imgL = cv2.imread(&#x27;image/A.jpg&#x27;)imgR = cv2.imread(&#x27;image/B.jpg&#x27;)# 转换为灰度图grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)# 提取特征点harris = cv2.xfeatures2d_HarrisLaplaceFeatureDetector.create()kL = harris.detect(grayL, None)kR = harris.detect(grayR, None)# 提取描述子br = cv2.BRISK_create()kL, dL = br.compute(grayL, kL)kR, dR = br.compute(grayR, kR)# 创建 BFMatcher 对象bf = cv2.BFMatcher(cv2.NORM_L2)# 根据描述子匹配特征点.matches = bf.match(dL, dR)# 画出匹配点img3 = cv2.drawMatches(imgL, kL, imgR, kR, matches, None, flags=2)cv2.imshow(&quot;Harris&quot;, img3)# ------------------------------------------------------------------------------------------------# 初始化Bruteforce匹配器bf = cv2.BFMatcher()# 通过KNN匹配两张图片的描述子matches = bf.knnMatch(dL, dR, k=2)# 筛选比较好的匹配点good = []for i, (m, n) in enumerate(matches): if m.distance &lt; 0.6 * n.distance: good.append(m)# 画出匹配点img3 = cv2.drawMatches(imgL, kL, imgR, kR, good, None, flags=2)cv2.imshow(&quot;Harris-BF&quot;, img3)cv2.waitKey(0)","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"基于SIFT的特征匹配","slug":"2019-03-24-cv-SIFT_feature","date":"2019-03-22T16:00:00.000Z","updated":"2022-03-25T13:38:26.621Z","comments":true,"path":"2019/03/23/2019-03-24-cv-SIFT_feature/","link":"","permalink":"https://aicc-cn.github.io/2019/03/23/2019-03-24-cv-SIFT_feature/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/3/2 22:26# @Author : Seven# @File : ImageMatching-SIFT.py# @Software: PyCharm# function : 使用SIFT进行提取特征点进行图片匹配import cv2# 加载图片imgL = cv2.imread(&#x27;image/A.jpg&#x27;)imgR = cv2.imread(&#x27;image/B.jpg&#x27;)# 转换为灰度图grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)# 提取特征点sift = cv2.xfeatures2d_SIFT.create()kL, dL = sift.detectAndCompute(grayL, None)kR, dR = sift.detectAndCompute(grayR, None)# 创建 BFMatcher 对象bf = cv2.BFMatcher(cv2.NORM_L2)# 根据描述子匹配特征点.matches = bf.match(dL, dR)# 画出匹配点img3 = cv2.drawMatches(imgL, kL, imgR, kR, matches, None, flags=2)cv2.imshow(&quot;SIFT&quot;, img3)# ------------------------------------------------------------------------------------------------# 设置FLANN 超参数FLANN_INDEX_KDTREE = 0# K-D树索引超参数index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)# 搜索超参数search_params = dict(checks=50)# 初始化FlannBasedMatcher匹配器flann = cv2.FlannBasedMatcher(index_params, search_params)# 通过KNN的方式匹配两张图的描述子matches = flann.knnMatch(dL, dR, k=2)# 筛选比较好的匹配点good = []for i, (m, n) in enumerate(matches): if m.distance &lt; 0.6 * n.distance: good.append(m)# 画出匹配点img3 = cv2.drawMatches(imgL, kL, imgR, kR, good, None, flags=2)cv2.imshow(&quot;SIFT-FLANN&quot;, img3)cv2.waitKey(0)","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"基于FAST的特征匹配","slug":"2019-03-21-cv-fast_feature","date":"2019-03-22T16:00:00.000Z","updated":"2022-03-25T13:52:43.451Z","comments":true,"path":"2019/03/23/2019-03-21-cv-fast_feature/","link":"","permalink":"https://aicc-cn.github.io/2019/03/23/2019-03-21-cv-fast_feature/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/3/4 13:57# @Author : Seven# @File : ImageMatching-FAST.py# @Software: PyCharm# function : 使用FAST进行提取特征点进行图片匹配import cv2# 加载图片imgL = cv2.imread(&#x27;image/A.jpg&#x27;)imgR = cv2.imread(&#x27;image/B.jpg&#x27;)# 转换为灰度图grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)# 提取特征点fast = cv2.FastFeatureDetector_create(50)kL = fast.detect(grayL, None)kR = fast.detect(grayR, None)# 提取描述子br = cv2.BRISK_create()kL, dL = br.compute(grayL, kL)kR, dR = br.compute(grayR, kR)# 创建 BFMatcher 对象bf = cv2.BFMatcher(cv2.NORM_L2)# 根据描述子匹配特征点.matches = bf.match(dL, dR)# 画出匹配点img3 = cv2.drawMatches(imgL, kL, imgR, kR, matches, None, flags=2)cv2.imshow(&quot;FAST&quot;, img3)# ------------------------------------------------------------------------------------------------# 初始化Bruteforce匹配器bf = cv2.BFMatcher()# 通过KNN匹配两张图片的描述子matches = bf.knnMatch(dL, dR, k=2)# 筛选比较好的匹配点good = []for i, (m, n) in enumerate(matches): if m.distance &lt; 0.6 * n.distance: good.append(m)# 画出匹配点img3 = cv2.drawMatches(imgL, kL, imgR, kR, good, None, flags=2)cv2.imshow(&quot;FAST-BF&quot;, img3)cv2.waitKey(0)","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【十三】背景建模","slug":"2019-03-13-cv-image_background","date":"2019-03-22T16:00:00.000Z","updated":"2022-03-25T13:51:56.365Z","comments":true,"path":"2019/03/23/2019-03-13-cv-image_background/","link":"","permalink":"https://aicc-cn.github.io/2019/03/23/2019-03-13-cv-image_background/","excerpt":"","text":"背景建模相对运动的基本方式 相机静止，目标运劢——背景提取(减除) 相机运劢，目标静止——光流估计(全局运劢) 相机和目标均运劢——光流估计 帧差法运动目标检测 ​ 如图可见，由目标运动引起的运动变化区域包括运动目标在前后两帧中的共同位置(图中黑色区域)、在当前帧中新显露出的背景区域和新覆盖的背景区域三部分。 【数学模型】$sign(x)&#x3D;\\begin{cases}1,&amp;如果I(x， y， t)-I(x, y, t-1)&gt;T \\ 0,&amp;如果其他情况\\end{cases}$ D(x, y): 帧差 I(x,y,t): 当前帧(t时刻)图像 I(x,y,t): 上一帧(t-1时刻)图像 T: 像素灰度差阈值 混合高斯背景建模【高斯背景】 像素灰度值随时间变化符合高斯分布. $I(x, y) \\thicksim N(u, \\sigma^2)$ 如果$I(x, y, t)-u&gt;3\\sigma$ 为前景，否则就是背景。 $I(x, y, t)$是当前帧。 $u$为均值、$\\sigma$是方差。 注意：当前帧的灰度值变化落在$3\\sigma$间的就是背景，否则就是前景。 【混合高斯模型】 任何一种分布函数都可以看做是多个高斯分布的组合. ​ 混合高斯背景建模是基于像素样本统计信息的背景表示方法，利用像素在较长时间内大量样本值的概率密度等统计信息(如模式数量、每个模式的均值和标准差)表示背景，然后使用统计差分(如3σ原则)进行目标像素判断，可以对复杂动态背景进行建模，计算量较大。​ 在混合高斯背景模型中，认为像素之间的颜色信息互不相关，对各像素点的处理都是相互独立的。对于视频图像中的每一个像素点，其值在序列图像中的变化可看作是不断产生像素值的随机过程，即用高斯分布来描述每个像素点的颜色呈现规律&#123;单模态(单峰)，多模态(多峰)&#125;。 ​ 对于多峰高斯分布模型，图像的每一个像素点按不同权值的多个高斯分布的叠加来建模，每种高斯分布对应一个可能产生像素点所呈现颜色的状态，各个高斯分布的权值和分布参数随时间更新。当处理彩色图像时，假定图像像素点R、G、B三色通道相互独立并具有相同的方差。对于随机变量X的观测数据集${x_1,x_2,…,x_N}，x_t&#x3D;(r_t,g_t,b_t)$为t时刻像素的样本，则单个采样点$x_t$其服从的混合高斯分布概率密度函数： ​ 其中Q为分布模式总数，$N(I,u_q, \\sigma_q^2)$为高斯分布，$μ_q$为其均值，$δ_q$为方差，I为三维单位矩阵，$ω_q$为高斯分布的权重。 【混合高斯背景建模步骤】 任务：在线计算：$μ_q$，$δ_q$，$ω_q$ 模型初始化 将采到的第一帧图像的每个象素的灰度值作为均值，再赋以较大的方差。初值Q&#x3D;1, w&#x3D;1.0。 模型学习 将当前帧的对应点象素的灰度值与已有的Q个高斯模型作比较，若满足$|x_k-u_{q, k}|&lt;2.5\\sigma_{q,k}$ ，则按概率密度公式调整第q个高斯模型的参数和权重；否则转入(3)： 增加&#x2F;替换高斯分量 若不满足条件，且q&lt;Q，则增加一个新分量；若q&#x3D;Q，则替换 判断背景 $B &#x3D; argmin_b(\\sum_{q&#x3D;1}^bw_q&gt;T)$ 判断前景 【混合高斯模型迭代计算原理】迭代计算： $M_q(k)$为二值化函数，仅当像素值匹配第q类时取1，其余取0 类别数值取值不大于5 总结 背景静止时，可以使用基于背景提取的运动估计斱法计算运动目标。 混合高斯模型可模拟任意概率密度函数，是背景建模的主流方法 混合高斯模型参数采用迭代方式计算。 代码实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/1/22 21:09# @Author : Seven# @File : backgroundDemo.py# @Software: PyCharm# function : 基于背景提取的运动估计import cv2# 加载视频cap = cv2.VideoCapture()cap.open(&#x27;768x576.avi&#x27;)if not cap.isOpened(): print(&quot;无法打开视频文件&quot;)pBgModel = cv2.createBackgroundSubtractorMOG2()def labelTargets(img, mask, threshold): seg = mask.copy() cnts = cv2.findContours(seg, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) count = 0 for i in cnts[1]: area = cv2.contourArea(i) if area &lt; threshold: continue count += 1 rect = cv2.boundingRect(i) print(&quot;矩形：X:&#123;&#125; Y:&#123;&#125; 宽：&#123;&#125; 高：&#123;&#125;&quot;.format(rect[0], rect[1], rect[2], rect[3])) cv2.drawContours(img, [i], -1, (255, 255, 0), 1) cv2.rectangle(img, (rect[0], rect[1]), (rect[0] + rect[2], rect[1] + rect[3]), (0, 0, 255), 1) cv2.putText(img, str(count), (rect[0], rect[1]), cv2.FONT_HERSHEY_PLAIN, 0.5, (0, 255, 0)) return countwhile True: flag, source = cap.read() if not flag: break image = cv2.pyrDown(source) fgMask = pBgModel.apply(image) kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5)) morphImage_open = cv2.morphologyEx(fgMask, cv2.MORPH_OPEN, kernel, iterations=5) mask = fgMask - morphImage_open _, Mask = cv2.threshold(mask, 30, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) # Mask = cv2.GaussianBlur(Mask, (5, 5), 0) targets = labelTargets(image, Mask, 30) print(&quot;共检测%s个目标&quot; % targets) backGround = pBgModel.getBackgroundImage() foreGround = image - backGround cv2.imshow(&#x27;source&#x27;, image) cv2.imshow(&#x27;background&#x27;, backGround) cv2.imshow(&#x27;foreground&#x27;, Mask) key = cv2.waitKey(10) if key == 27: break","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"基于ORB的特征匹配","slug":"2019-03-22-cv-ORB_feature","date":"2019-03-22T16:00:00.000Z","updated":"2022-03-25T13:38:18.191Z","comments":true,"path":"2019/03/23/2019-03-22-cv-ORB_feature/","link":"","permalink":"https://aicc-cn.github.io/2019/03/23/2019-03-22-cv-ORB_feature/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/3/4 12:54# @Author : Seven# @File : ImageMatching-ORB.py# @Software: PyCharm# function : 使用ORB进行提取特征点进行图片匹配import cv2# 加载图片imgL = cv2.imread(&#x27;image/A.jpg&#x27;)imgR = cv2.imread(&#x27;image/B.jpg&#x27;)# 转换为灰度图grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)# 提取特征点orb = cv2.ORB_create()kL, dL = orb.detectAndCompute(grayL, None)kR, dR = orb.detectAndCompute(grayR, None)# 创建 BFMatcher 对象bf = cv2.BFMatcher(cv2.NORM_L2)# 根据描述子匹配特征点.matches = bf.match(dL, dR)# 画出匹配点img3 = cv2.drawMatches(imgL, kL, imgR, kR, matches, None, flags=2)cv2.imshow(&quot;ORB&quot;, img3)# ------------------------------------------------------------------------------------------------# 初始化Bruteforce匹配器bf = cv2.BFMatcher()# 通过KNN匹配两张图片的描述子matches = bf.knnMatch(dL, dR, k=2)# 筛选比较好的匹配点good = []for i, (m, n) in enumerate(matches): if m.distance &lt; 0.6 * n.distance: good.append(m)# 画出匹配点img3 = cv2.drawMatches(imgL, kL, imgR, kR, good, None, flags=2)cv2.imshow(&quot;ORB-BF&quot;, img3)cv2.waitKey(0)","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"基于SURF的特征匹配","slug":"2019-03-25-cv-SURF_feature","date":"2019-03-22T16:00:00.000Z","updated":"2022-03-25T13:38:30.762Z","comments":true,"path":"2019/03/23/2019-03-25-cv-SURF_feature/","link":"","permalink":"https://aicc-cn.github.io/2019/03/23/2019-03-25-cv-SURF_feature/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/3/2 21:36# @Author : Seven# @File : ImageMatching-SURF.py# @Software: PyCharm# function : 使用SURF进行提取特征点进行图片匹配import cv2# 加载图片imgL = cv2.imread(&#x27;image/A.jpg&#x27;)imgR = cv2.imread(&#x27;image/B.jpg&#x27;)# 转换为灰度图grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)# 提取特征点surf = cv2.xfeatures2d.SURF_create(hessianThreshold=800) # hessian矩阵阈值，在这里调整精度，值越大，点越少，越精准kL, dL = surf.detectAndCompute(grayL, None)kR, dR = surf.detectAndCompute(grayR, None)# 创建 BFMatcher 对象bf = cv2.BFMatcher(cv2.NORM_L2)# 根据描述子匹配特征点.matches = bf.match(dL, dR)# 画出匹配点img3 = cv2.drawMatches(imgL, kL, imgR, kR, matches, None, flags=2)cv2.imshow(&quot;SURF&quot;, img3)# ------------------------------------------------------------------------------------------------# 设置FLANN 超参数FLANN_INDEX_KDTREE = 0# K-D树索引超参数index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)# 搜索超参数search_params = dict(checks=50)# 初始化FlannBasedMatcher匹配器flann = cv2.FlannBasedMatcher(index_params, search_params)# 通过KNN的方式匹配两张图的描述子matches = flann.knnMatch(dL, dR, k=2)# 筛选比较好的匹配点good = []for i, (m, n) in enumerate(matches): if m.distance &lt; 0.6 * n.distance: good.append(m)# 画出匹配点img3 = cv2.drawMatches(imgL, kL, imgR, kR, good, None, flags=2)cv2.imshow(&quot;SURF-FLANN&quot;, img3)cv2.waitKey(0)","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"图片拼接","slug":"2019-03-26-cv-IMAGE_JOIN","date":"2019-03-22T16:00:00.000Z","updated":"2022-03-25T13:38:40.199Z","comments":true,"path":"2019/03/23/2019-03-26-cv-IMAGE_JOIN/","link":"","permalink":"https://aicc-cn.github.io/2019/03/23/2019-03-26-cv-IMAGE_JOIN/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/2/16 21:29# @Author : Seven# @File : ImageJoin.py# @Software: PyCharm# function : 图片拼接import cv2import numpy as npdef get_stitched_image(img1, img2, M): &quot;&quot;&quot; 使用关键点来缝合图像 :param img1: :param img2: :param M: 对应矩阵 :return: 拼接后的图片数据 &quot;&quot;&quot; # 获取输入图片的维度 w1, h1 = img1.shape[:2] w2, h2 = img2.shape[:2] # 生成对应维度的画布 img1_dims = np.float32([[0, 0], [0, w1], [h1, w1], [h1, 0]]).reshape(-1, 1, 2) img2_dims_temp = np.float32([[0, 0], [0, w2], [h2, w2], [h2, 0]]).reshape(-1, 1, 2) # 获取透视变换的交换矩阵 img2_dims = cv2.perspectiveTransform(img2_dims_temp, M) # 最终的维度 result_dims = np.concatenate((img1_dims, img2_dims), axis=0) # 开始拼接图片 # 通过匹配点来计算维度 [x_min, y_min] = np.int32(result_dims.min(axis=0).ravel() - 0.5) [x_max, y_max] = np.int32(result_dims.max(axis=0).ravel() + 0.5) # 变换后创建输出矩阵 transform_dist = [-x_min, -y_min] transform_array = np.array([[1, 0, transform_dist[0]], [0, 1, transform_dist[1]], [0, 0, 1]]) # 修正图像以获得结果图像 result_img = cv2.warpPerspective(img2, transform_array.dot(M), (x_max - x_min, y_max - y_min)) result_img[transform_dist[1]:w1 + transform_dist[1], transform_dist[0]:h1 + transform_dist[0]] = img1 # 返回最终结果 return result_imgdef get_sift_homography(img1, img2): &quot;&quot;&quot; 使用SIFT方法获取两张图片的关键点 :param img1: :param img2: :return: 关键点组成的对应矩阵 &quot;&quot;&quot; # 初始化SIFT方法 sift = cv2.xfeatures2d_SIFT.create() # 获取关键点和描述子 k1, d1 = sift.detectAndCompute(img1, None) k2, d2 = sift.detectAndCompute(img2, None) # 初始化Bruteforce匹配器 bf = cv2.BFMatcher() # 通过KNN匹配两张图片的描述子 matches = bf.knnMatch(d1, d2, k=2) # 确定最好的匹配 verified_matches = [] for m1, m2 in matches: # 把匹配的较好的添加到矩阵中 if m1.distance &lt; 0.8 * m2.distance: verified_matches.append(m1) # 最小匹配数 min_matches = 8 if len(verified_matches) &gt; min_matches: # 存储匹配点 img1_pts = [] img2_pts = [] # 将匹配点加入矩阵中 for match in verified_matches: img1_pts.append(k1[match.queryIdx].pt) img2_pts.append(k2[match.trainIdx].pt) img1_pts = np.float32(img1_pts).reshape(-1, 1, 2) img2_pts = np.float32(img2_pts).reshape(-1, 1, 2) # 计算单应矩阵 M, mask = cv2.findHomography(img1_pts, img2_pts, cv2.RANSAC, 5.0) return M else: print(&#x27;Error: Not enough matches&#x27;) exit()def changeImage(img1, img2): # 使用SIFT查找关键点并返回关键点组成的矩阵 M = get_sift_homography(img1, img2) # 使用关键点矩阵将图像缝合在一起 result_image = get_stitched_image(img2, img1, M) return result_imagedef main(): # 加载图片 img1 = cv2.imread(&quot;images/csdn_A.jpg&quot;) img2 = cv2.imread(&quot;images/csdn_B.jpg&quot;) img3 = cv2.imread(&quot;images/csdn_C.jpg&quot;) # 显示加载的图片 input_images = np.hstack((img1, img2, img3)) cv2.imshow(&#x27;Input Images&#x27;, input_images) out1 = changeImage(img1, img2) out2 = changeImage(img1, img3) result_image = changeImage(out1, out2) # 保存文件 result_image_name = &#x27;results/result_tree.jpg&#x27; cv2.imwrite(result_image_name, result_image) # 显示拼接的结果 cv2.imshow(&#x27;Result&#x27;, result_image) cv2.waitKey()if __name__ == &#x27;__main__&#x27;: main()","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【一】计算机视觉引论","slug":"2019-03-01-cv-introduction","date":"2019-03-21T16:00:00.000Z","updated":"2022-03-25T13:50:50.123Z","comments":true,"path":"2019/03/22/2019-03-01-cv-introduction/","link":"","permalink":"https://aicc-cn.github.io/2019/03/22/2019-03-01-cv-introduction/","excerpt":"","text":"计算机视觉 计算机视觉就是让计算机看懂图像和视频。 视觉是自然智能不可思议的结晶 猕猴的大脑皮层中视觉部分占据大约50% 人脑中有关视觉的部分所占比重最大 人类大脑对视觉进行层次化的处理 人类采用神经网络对视觉信息进行深层次处理，和深度学习密切结合。 计算机视觉的产生和发展都经历的阶段 起源：20世纪50年代统计模式识别，二维图像分析。 诞生：1974年 Minsky -&gt; David Marr 暑期， 1981年，人工智能“计算机视觉”专辑，Marr视觉计算理论 得到了迅速的发展。 发展： 80年代以后： 随着计算能力的迅速增长，视觉计算成本极大降低。 以Marr理论为基础的视觉理论广泛研究。 2000年后，特征提取和基于学习的视觉得到迅速发展。 2006年，Hinton 提出深度学习。 2010年， 微软使用深度学习在语音方面取得突破进展。 2015年后，深度学习在视觉个各邻域取得突破： 2015年， 在imageNet上的识别准确率首次超越人类。 2016年，Tesla 创造了56亿公里的自动驾驶路测数据。 2017年，iPone x 宣布引入Face ID 高精度人脸识别技术。 2018年，OpenAI 2:1 战胜人类DOTA2高手队。 计算机视觉的应用 服务机器人 安防监控 自动驾驶 智能穿戴 无人机快递 等等 内容架构 遵循Marr视觉计算机理论 从初略图（2维）–&gt; $2 \\frac12$维 –&gt;3维 初略图 “看见”–照相，颜色，图像采集的过程。 “基本理解”– 滤波、边缘检测、灰度直方图、直线检测–基本特征提取 图像阈值分割、区域生长、图像描述 关键点及特征检测 背景建模及运动估计 $2 \\frac12$维 视觉成像模型、视觉几何基础、相机标定 3维 图像拼接 立体视觉 从底层–&gt;中层–&gt;高层 底层是由高等数学，线性代数，矩阵分析，概率论，最优化方法，物理(运动学)来进行架构 作为理论支撑。 中层：机器视觉是建立在数字图像处理和模式识别之上的。 高层：深度学习+计算机视觉 即将更新","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【二】视觉系统","slug":"2019-03-02-cv-visualsystem","date":"2019-03-21T16:00:00.000Z","updated":"2022-03-25T13:50:54.566Z","comments":true,"path":"2019/03/22/2019-03-02-cv-visualsystem/","link":"","permalink":"https://aicc-cn.github.io/2019/03/22/2019-03-02-cv-visualsystem/","excerpt":"","text":"视觉系统构成要素 照明设备：光源 成像设备：相机 处理设备：主机 算法软件：视觉处理系统 【要素的关系】 【要素的理解】 光源：对场景进行照明，使能捕捉的范围更大，事物更清晰，一般就是各种光。 相机：抓取图片，保留信息，一般是指照相机，摄像机，一个或多个。 主机：处理图片信息，一般为台式计算机或嵌入式处理器。 算法软件：辅助主机处理图片信息，提取所需要的特征， 一般为C++或者其他编程语言编写的视觉识别算法及程序。 【要素案例】 光源：室内光线或专用照明 相机：放在机械臂前端，单相机 主机：台式计算机或嵌入式处理器 算法软件：使用C++或其它语言编写的视觉识别算法及程序 光源：红外和可见光 相机：RGB+红外 主机：类PC结构 软件：运行在SoC+后端主机软件 Marr视觉计算机理论 目的：通过视觉系统，重构三维物体的形状和位置 初略图（2维）：过零点（zero-crossing）、短线段、端点等基本特征 $2 \\frac12$维：对物体形状的一些初略描述 3维：对物体的三维描述 【解释】 Marr视觉计算机理论就是通过视觉捕捉物体的图像和位置，然后通过技术重构物体的三维特征；这个过程包括： 先通过图像提取出一些2维的初略图 然后对物体的形状的特征(法向量等)抽取并做一些初略的描述确定$2 \\frac12$维图 最后综合所有特征形成物体的三维特征图。","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【三】数字成像系统","slug":"2019-03-03-cv-digitaltoimage","date":"2019-03-21T16:00:00.000Z","updated":"2022-03-25T13:51:00.893Z","comments":true,"path":"2019/03/22/2019-03-03-cv-digitaltoimage/","link":"","permalink":"https://aicc-cn.github.io/2019/03/22/2019-03-03-cv-digitaltoimage/","excerpt":"","text":"光通量 指人眼所能感觉到的辐射功率，它等于单位时间内某一波段的辐射能量和该波段的相对视见率的乘积。 符号：Φ 单位：lm(流明) 1lm &#x3D; 0.00146瓦 常见光源的光通量 辐照度 指投射到一平方米表面上的辐射通量密度。也就是说是到达一平方米表面上，单位时间，单位面积上的辐射能。 符号：E 单位：lux(勒克斯) 1 lux &#x3D; 1 lm&#x2F;$m^2$ 常见照明环境的辐照度 光源类别 按方向 直射光 漫射光 按光谱 可见光 近可见光 其他 偏振 其他 特点【透视轮廓–背光源】 发射面是一个漫射面， 均匀性好。可用于镜面反射材料，如晶片或者玻璃基底上的伤痕检测；LCD检测；微小电子元件尺寸、形状、靶标测试 【表面照明–漫射光源】 在一定工作距离下，光束集中、亮度高、均匀性好、照射面积相对较小。常用于液晶校正、塑胶容器检查、工作螺孔定位、标签检查、管脚检查、集成电路印字检查等等 【颜色光源】 如果希望更加鲜明地突出某些颜色，则选择色环上相对应的互补颜色光源照明，这样就可以明显的提高图像的对比度。 RGB 为生活中所最常见的三基色，也是和视觉感受一一致的，很多颜色都是由这三基色产生。 三基色: 红、绿、蓝 三色相交互是白色。 CYMK 补色模型 基色变为基本三基色的补色–红色–&gt;青色、绿色–&gt;黄色、蓝色–&gt;品红 三基色：青、黄、品红 三色交互是黑色。 HSI 色调H：描述纯色的属性（红、黄）。 饱和度S：表示的是一种纯色被白光稀释的程度的度量。 亮度I：体现了无色的光照强度的概念，是一个主观的描述。 【与RGB的换算关系】 代码演示 1234567891011121314151617181920212223242526272829import cv2# 加载图片img = cv2.imread(&#x27;2.jpg&#x27;)# 缩放+灰度化sizeImage = cv2.pyrDown(img)grayImage = cv2.cvtColor(sizeImage, cv2.COLOR_BGR2GRAY)cv2.imshow(&quot;source image&quot;, sizeImage)cv2.imshow(&quot;gray&quot;, grayImage)# RGB通道分离--opencv中，RGB三个通道是反过来的rgbImage = cv2.split(sizeImage)cv2.imshow(&quot;R&quot;, rgbImage[2])cv2.imshow(&quot;G&quot;, rgbImage[1])cv2.imshow(&quot;B&quot;, rgbImage[0])# 分离后为单通道，相当于分离通道的同时把其他两个通道填充了相同的数值。# 比如红色通道，分离出红色通道的同时，绿色和蓝色被填充为和红色相同的数值，这样一来就只有黑白灰了。# 可以进行观察，会发现原图中颜色越接近红色的地方在红色通道越接近白色。# 在纯红的地方在红色通道会出现纯白。# R值为255 -&gt; RGB(255，255，255)，为纯白# HSI颜色模型+通道分离hsv = cv2.cvtColor(sizeImage, cv2.COLOR_BGR2HSV)hsvChannels = cv2.split(hsv)cv2.imshow(&quot;Hue&quot;, hsvChannels[0])cv2.imshow(&quot;Saturation&quot;, hsvChannels[1])cv2.imshow(&quot;Value&quot;, hsvChannels[2])cv2.waitKey(0) CCD传感器基本原理 彩色图像传感器的基本原理 最底层的感光区同样是采用CCD图像传感器 彩色滤色片阵列(Color Filter Array),也被称为拜尔滤色镜(Bayer Filter)，排列在感光区上方。 传感器不同，排列方式就不同。 这个单元包含了三原色：红(R),绿(G),蓝(B)。对于单个像素点，只有一种特定的彩色滤色片放置在其上方。彩色图像的像素点不仅包含通常的感光区域还包括了一个彩色滤色片。因为人眼对绿色的敏感度很高，所以我们使用了更多的绿色滤色片，类似棋盘格的形式来摆放绿色滤色片，以期得到有着更高空间分辨率的图像。红色和蓝色滤色片每隔一行与绿色滤色片错落放置。 γ校正 图像传感器输出时经过γ校正，以符合人的视觉习惯；存储时还原回原有的RGB值。 图像传输的方式 模拟视频传输：采用同轴电缆等方式，将亮度和色度分离，在不同频带调制后在同一信号线上传输。常用的为同轴电缆，同轴电缆的中心导线用于传输信号，外层是金属屏蔽网。 RGB方式：显示器，投影 数字传输（长距离）：光纤高清信号，网线 数字传输（短距离）：USB，火线，HDMI 模拟、数字视频传输接口 常见的图像和视频压缩标准 显示设备及参数（液晶显示） 分辨率（最佳分辨率）及显示器尺寸 亮度和对比度：LCD的亮度以流明&#x2F;平方米（cd&#x2F;m2）度量，对比度是直接反映LCD显示器能否现丰富的色阶的参数 响应时间：响应时间是LCD显示器的一个重要的参数，它指的是LCD显示器对于输入信号的反应时间。 坏点：如果液晶显示屏中某一个发光单元有问题就会出现总丌透光、总透光、半透光等现象，这就是所谓的“坏点”","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【五】图像边缘检测","slug":"2019-03-05-cv-image_edge","date":"2019-03-21T16:00:00.000Z","updated":"2022-03-25T13:51:10.104Z","comments":true,"path":"2019/03/22/2019-03-05-cv-image_edge/","link":"","permalink":"https://aicc-cn.github.io/2019/03/22/2019-03-05-cv-image_edge/","excerpt":"","text":"基本思想 边缘检测的基本思想是通过检测每个像素和其邻域的状态，以决定该像素是否位于一个物体的边界上。如果一个像素位于一个物体的边界上，则其邻域像素的灰度值的变化就比较大。假如可以应用某种算法检测出这种变化并进行量化表示，那么就可以确定物体的边界。 边缘检测的实质是微分。 实际中常用差分， X方向、Y方向。 基本算子【Robert算子：一阶微分算子】对于图像来说，是一个二维的离散型数集，通过推广二维连续型求函数偏导的方法，来求得图像的偏导数，即在(x,y)处的最大变化率，也就是这里的梯度： 梯度是一个矢量，则(x,y)处的梯度表示为： 其大小为： 因为平方和平方根需要大量的计算开销，所以使用绝对值来近似梯度幅值： 方向与α(x,y)正交： 其对应的模板为： 图像的垂直和水平梯度 我们有时候也需要对角线方向的梯度，定义如下： 对应模板为： Roberts 交叉梯度算子。 模板推导 垂直或水平算子模板 交叉算子模板 2*2 大小的模板在概念上很简单，但是他们对于用关于中心点对称的模板来计算边缘方向不是很有用，其最小模板大小为3*3。3*3 模板考虑了中心点对段数据的性质，并携带有关于边缘方向的更多信息。 【Prewitt算子：一阶微分算子】 prewitt算子一般使用的是3*3的模板 我如下定义水平、垂直和两对角线方向的梯度: Prewitt算子： 数学推导： Prewitt 算子是由两部分组成，检测水平边缘的模板和检测垂直边缘的模板， Prewitt 算子一个方向求微分，一个方向求平均，所以对噪声相对不敏感。 【Sobel算子：一阶微分算子】 Sobel 算子是在Prewitt算子的基础上改进的， 在中心系数上使用一个权值2， 相比较Prewitt算子，Sobel模板能够较好的抑制(平滑)噪声。 计算公式： sobel算子： 数学推导： sobel算子也有两个，一个是检测水平边缘的模板基于sobel算子的边缘检测，另一个是检测垂直边缘的模板基于sobel算子的边缘检测。 与Prewitt算子相比，sobel算子对于像素位置的影响做了加权，因此效果更好。 sobel算子的另外一种形式是各向同性Sobel算子， 也有两个模板组成，一个是检测水平边缘的基于sobel算子的边缘检测，另一个是检测垂直边缘的基于sobel算子的边缘检测。 各向同性Sobel算子和普通sobel算子相比，位置加权系数更为准确，在检测不同方向的边缘是梯度的幅度是一致的。 【Laplace算子：二阶微分算子】 Laplace 算子是一种各向同性算子，在只关心边缘的位置而不考虑其周围的像素灰度差值时比较合适。Laplace算子对孤立像素的响应要比对边缘或线的响应要更强烈， 因此只适合用于无噪声图像。存在噪声情况下， 使用Laplace算子检测边缘之前需要先进行低通滤波。 一阶导数： 二阶导数： 我们需要注意的是关于点x的二阶导数，故将上式中的变量减去1后，得到： 在图像处理中通过拉普拉斯模板求二阶导数， 其定义如下： 写成差分形式为 laplace卷积核： 在用lapacian 算子图像进行卷积运算时，当响应的绝对值超过指定阈值时，那么该点就是被检测出来的孤立点，具体输出如下： 【LoG算子：二阶微分算子】 Log 边缘检测则是先进行高斯滤波再进行拉普拉斯算子检测, 然后找过零点来确定边缘位置，很多时候我们只是知道Log 5*5 模板如上图所示。 二维高斯公式： 按拉普拉斯算子公式求X，Y方向的二阶偏导后： 这里x，y 不能看成模板位置，应看成是模板其他位置到中心位置的距离。那么写成： 这里x0，y0 就是模板中心位置，x，y 是模板其他位置，对于5*5 模板， 则$x_0,&#x3D;2 , y_0&#x3D;2$，那对于模板中（0,0）位置的权值，即把x&#x3D; 0，y&#x3D; 0，x0&#x3D; 2，y0 &#x3D; 2 带入上式， 令σ&#x3D; 1，得到约等于0.0175，这样得到 通过取整变符号，且模板总和为0，得到下图所示的模板。 通常高斯分布中，在（-3σ，3σ）的范围内就覆盖了绝大部分区域，所以模板大小一般取dim &#x3D; 1 + 6σ（在SIFT 特征中，其中的高斯模糊也是这样取），dim 如果为小数，则取不小于dim 的最小整数，当然实际使用时没有这么严格，如上面我们取σ&#x3D;1 时，模板大小取5*5。那同一个尺寸的模板中的权值调整就是σ的变化得到的，变化到一定程度，模板尺寸大小改变。 Canny算子：非微分算子【基本原理】 图象边缘检测必须满足两个条件: 能有效的抑制噪声。 必须尽量精确确定边缘的位置。 根据对信噪比与定位乘积进行测度，，得到最优化逼近算子。这就是Canny边缘检测算子。 类似与LoG边缘检测方法，也属于先平滑后求导数的方法。 【算法步骤】 使用高斯滤波器，以平滑图像，滤除噪声。 计算图像中每个像素点的梯度强度和方向。 应用非极大值（Non-Maximum Suppression）抑制， 以消除边缘检测带着的杂散响应。 应用双阈值（Double-Threshold）检测来确定真实的和潜在的边缘。 通过抑制孤立的弱边缘最终完成边缘检测。 第一步：同时平滑与微分 使用高斯函数的一阶导数同时完成平滑和微分。 第二步：计算梯度和方向 梯度幅值和方向 方向离散化：离散化为上下左右和斜45°共4个方向 第三步：梯度幅值非极大值抑制细化梯度幅值图像中的屋脊带，只保留幅值局部变化最大的点 使用一个3*3邻域作用于幅值阵列的所有点。在每一点上, 邻域的中心像素与沿梯度方向的两个梯度幅值的插值结果进行较，仅保留极大值点 第四步：边缘连接对上一步得到的图像使用低、高阈值t1, t2 阈值化，得到三幅图像 T1对应假边缘，去除 T3对应真边缘，全部保留 T2连接：临接像素中是否有属于T3的像素 第五步：抑制孤立低阈值点 边缘检测总结 边缘检测即图像差分 常见边缘检测算子包括Robert算子，Sobel算子，LoG算子等，其中Sobel算子最为常用 Canny算子的基本优点在于检测准确、对噪声稳健，在实际中广泛应用. 代码演示123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152import cv2import numpy as np# 加载图片image = cv2.imread(&#x27;lena.jpg&#x27;)img = cv2.pyrDown(image)cv2.imshow(&quot;source image&quot;, img)def RobertImage(img, name): &quot;&quot;&quot; robert算子的实现 :param img: :param name: :return: &quot;&quot;&quot; r_sunnzi = np.array([[-1, 1], [1, -1]], np.float32) robertImage = cv2.filter2D(img, -1, r_sunnzi) cv2.imshow(&quot;Robert-%s&quot; % name, robertImage)def sobelImage(img, name): &quot;&quot;&quot; Sobel算子 Sobel(src, ddepth, dx, dy, dst=None, ksize=None, scale=None, delta=None, borderType=None) 前四个是必须的参数： 第一个参数是需要处理的图像； 第二个参数是图像的深度，-1表示采用的是与原图像相同的深度。目标图像的深度必须大于等于原图像的深度； dx和dy表示的是求导的阶数，0表示这个方向上没有求导，一般为0、1、2。 其后是可选的参数： dst不用解释了； ksize是Sobel算子的大小，必须为1、3、5、7。 scale是缩放导数的比例常数，默认情况下没有伸缩系数； delta是一个可选的增量，将会加到最终的dst中，同样，默认情况下没有额外的值加到dst中； borderType是判断图像边界的模式。这个参数默认值为cv2.BORDER_DEFAULT。 :param img: 图片数据 :param name: 命名标志 :return: &quot;&quot;&quot; # 先检测xy方向上的边缘 Sobel_Ex = cv2.Sobel(src=img, ddepth=cv2.CV_16S, dx=1, dy=0, ksize=3) Sobel_Ey = cv2.Sobel(src=img, ddepth=cv2.CV_16S, dx=0, dy=1, ksize=3) # 即Sobel函数求完导数后会有负值，还有会大于255的值。而原图像是uint8，即8位无符号数， # 所以Sobel建立的图像位数不够，会有截断。因此要使用16位有符号的数据类型，即cv2.CV_16S。 # cv2.imshow(&quot;Sobel_Ex-%s&quot; % name, Sobel_Ex) # 图像的每一个像素的横向及纵向梯度近似值结合 # 用convertScaleAbs()函数将其转回原来的uint8形式。否则将无法显示图像，而只是一副灰色的窗口。 absX = cv2.convertScaleAbs(Sobel_Ex) absY = cv2.convertScaleAbs(Sobel_Ey) # cv2.imshow(&quot;absX-%s&quot; % name, absX) # cv2.imshow(&quot;absY-%s&quot; % name, absY) SobelImage = cv2.addWeighted(absX, 0.5, absY, 0.5, 0) cv2.imshow(&quot;sobel-%s&quot; % name, SobelImage) # ddepth=-1表示图像深度和原图深度一致 # Ex = cv2.Sobel(img, -1, 1, 0, ksize=3) # Ey = cv2.Sobel(img, -1, 0, 1, ksize=3) # # cv2.imshow(&quot;Ex-%s&quot; % name, Ex) # SobelImg = cv2.addWeighted(Ex, 0.5, Ey, 0.5, 0) # cv2.imshow(&quot;soImg-%s&quot; % name, SobelImg)def LaplaceImage(img, name): &quot;&quot;&quot; Laplace算子 Laplacian(src, ddepth, dst=None, ksize=None, scale=None, delta=None, borderType=None) 前两个是必须的参数： 第一个参数是需要处理的图像； 第二个参数是图像的深度，-1表示采用的是与原图像相同的深度。目标图像的深度必须大于等于原图像的深度； 其后是可选的参数： dst不用解释了； ksize是算子的大小，必须为1、3、5、7。默认为1。 scale是缩放导数的比例常数，默认情况下没有伸缩系数； delta是一个可选的增量，将会加到最终的dst中，同样，默认情况下没有额外的值加到dst中； borderType是判断图像边界的模式。这个参数默认值为cv2.BORDER_DEFAULT。 :param img: 图片数据 :param name: 命名标志 :return: &quot;&quot;&quot; # ddepth=-1表示图像深度和原图深度一致 # laplaceImage = cv2.Laplacian(img, ddepth=-1, ksize=3) # cv2.imshow(&quot;laplace-%s&quot; % name, laplaceImage) # ddepth=cv2.CV_16S表示图像深度 laplaceImg = cv2.Laplacian(img, ddepth=cv2.CV_16S, ksize=3) lapimg = cv2.convertScaleAbs(laplaceImg) cv2.imshow(&quot;laplace-%s&quot; % name, lapimg)def LoG(img, name): &quot;&quot;&quot; LoG算子的实现 :param img: :param name: :return: &quot;&quot;&quot; gaussImage = cv2.GaussianBlur(img, (5, 5), 0.01) laplaceImg = cv2.Laplacian(gaussImage, ddepth=cv2.CV_16S, ksize=3) logImg = cv2.convertScaleAbs(laplaceImg) cv2.imshow(&quot;LoG-%s&quot; % name, logImg)def CannyImage(img, name): &quot;&quot;&quot; 必要参数： 第一个参数是需要处理的原图像，该图像必须为单通道的灰度图； 第二个参数是阈值1； 第三个参数是阈值2。 其中较大的阈值2用于检测图像中明显的边缘，但一般情况下检测的效果不会那么完美， 边缘检测出来是断断续续的。所以这时候用较小的第一个阈值用于将这些间断的边缘连接起来。 :param img: 图片数据 :param name: 命名标志 :return: &quot;&quot;&quot; # 先进行高斯平滑 gaussImage = cv2.GaussianBlur(img, (3, 3), 0) # 边缘检测，最大阈值为150，最小阈值为50 # Canny 推荐的 高:低 阈值比在 2:1 到3:1之间。 cannyImage = cv2.Canny(gaussImage, 50, 150) cv2.imshow(&quot;canny-%s&quot; % name, cannyImage)# 彩色图像进行Robert边缘检测RobertImage(img, &#x27;rgb&#x27;)# 灰度图像进行robert边缘检测grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)RobertImage(grayImage, &#x27;gray&#x27;)# 彩色图像进行sobel边缘检测sobelImage(img, &#x27;rgb&#x27;)# 灰度图像进行sobel边缘检测grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)sobelImage(grayImage, &#x27;gray&#x27;)# 彩色图像进行laplace边缘检测LaplaceImage(img, &#x27;rgb&#x27;)# 灰度图像进行laplace边缘检测grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)LaplaceImage(grayImage, &#x27;gray&#x27;)# 彩色图像进行LoG边缘检测LoG(img, &#x27;rgb&#x27;)# 灰度图像进行LoG边缘检测grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)LoG(grayImage, &#x27;gray&#x27;)# 彩色图像进行canny边缘检测CannyImage(img, &#x27;rgb&#x27;)# 灰度图像进行canny边缘检测grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)CannyImage(grayImage, &#x27;gray&#x27;)cv2.waitKey(0) sobel 产生的边缘有强弱，抗噪性好 laplace 对边缘敏感，可能有些是噪声的边缘，也被算进来了 canny 产生的边缘很细，可能就一个像素那么细，没有强弱之分。","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【六】直方图","slug":"2019-03-06-cv-image_hist","date":"2019-03-21T16:00:00.000Z","updated":"2022-03-25T13:51:15.610Z","comments":true,"path":"2019/03/22/2019-03-06-cv-image_hist/","link":"","permalink":"https://aicc-cn.github.io/2019/03/22/2019-03-06-cv-image_hist/","excerpt":"","text":"图像直方图 通过灰度直方图看到图像的照明效果 代码实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/3/9 21:28# @Author : Seven# @File : HistDemo.py# @Software: PyCharm# function : 图像直方图import cv2import matplotlib.pyplot as pltimport numpy as npdef GrayHist(img): &quot;&quot;&quot; calcHist(images, channels, mask, histSize, ranges, hist=None, accumulate=None) images — 源矩阵指针（输入图像，可以多张）。 但都应当有同样的深度（depth）, 比如CV_8U 或者 CV_32F ， 和同样的大小。每张图片可以拥有任意数量的通道。 channels — 通道的数量，每个通道都有编号，灰度图为一通道，即channel[1]=0;对应着一维。 BGR三通道图像编号为channels[3]=&#123;0,1,2&#125;;分别对应着第一维，第二维，第三维。 mask — 掩码图像：要对图像处理时，先看看这个像素对应的掩码位是否为屏蔽，如果为屏蔽，就是说该像素不处理（掩码值为0的像素都将被忽略） hist — 返回的直方图。 histSize — 直方图每一维的条目个数的数组，灰度图一维有256个条目。 ranges — 每一维的像素值的范围，传递数组，与维数相对应，灰度图一维像素值范围为0~255。 :param name: 命名标志 :param img: 图片数据 :return: &quot;&quot;&quot; img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) hist = cv2.calcHist([img], [0], None, [256], [0.0, 255.0]) plt.plot(hist) plt.xlim([0, 256]) plt.show()def colorHist(img): # 颜色直方图 channels = cv2.split(img) colors = (&#x27;b&#x27;, &#x27;g&#x27;, &#x27;r&#x27;) for (chan, color) in zip(channels, colors): hist = cv2.calcHist([chan], [0], None, [256], [0, 256]) plt.plot(hist, color=color) plt.xlim([0, 256]) plt.show()def channelsHist(img): b, g, r = cv2.split(img) def calcAndDrawHist(image, color): hist = cv2.calcHist([image], [0], None, [256], [0.0, 255.0]) minVal, maxVal, minLoc, maxLoc = cv2.minMaxLoc(hist) histImg = np.zeros([256, 256, 3], np.uint8) hpt = int(0.9 * 256); for h in range(256): intensity = int(hist[h] * hpt / maxVal) cv2.line(histImg, (h, 256), (h, 256 - intensity), color) return histImg; histImgB = calcAndDrawHist(b, [255, 0, 0]) histImgG = calcAndDrawHist(g, [0, 255, 0]) histImgR = calcAndDrawHist(r, [0, 0, 255]) plt.figure(figsize=(8, 6)) plt.subplot(221) plt.imshow(img[:, :, ::-1]); plt.title(&#x27;origin&#x27;) plt.subplot(222) plt.imshow(histImgB[:, :, ::-1]); plt.title(&#x27;histImgB&#x27;) plt.subplot(223) plt.imshow(histImgG[:, :, ::-1]); plt.title(&#x27;histImgG&#x27;) plt.subplot(224) plt.imshow(histImgR[:, :, ::-1]); plt.title(&#x27;histImgR&#x27;) plt.tight_layout() plt.show()def grayEqualize(img): img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) equ = cv2.equalizeHist(img) # 两个图片的像素分布连接在一起，拍成一维数组 res = np.hstack((img, equ)) plt.figure(figsize=(8, 6)) ax1 = plt.subplot2grid((2, 2), (0, 0)) ax1.imshow(img, cmap=plt.cm.gray) ax1.set_title(&#x27;orignal image&#x27;) ax1 = plt.subplot2grid((2, 2), (0, 1)) ax1.imshow(equ, cmap=plt.cm.gray) ax1.set_title(&#x27;equalization&#x27;) ax1 = plt.subplot2grid((2, 2), (1, 0), colspan=3, rowspan=1) ax1.imshow(res, cmap=plt.cm.gray) ax1.set_title(&#x27;horizational&#x27;) plt.tight_layout() plt.show()def colorEqualize(img): def hisEqulColor(img): ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB) channels = cv2.split(ycrcb) print(len(channels)) cv2.equalizeHist(channels[0], channels[0]) cv2.merge(channels, ycrcb) cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR, img) return img equ = hisEqulColor(img) h_stack_img = np.hstack((img, equ)) plt.figure(figsize=(10, 8)) ax1 = plt.subplot2grid((2, 2), (0, 0)) ax1.imshow(img[:, :, ::-1]) ax1.set_title(&#x27;orignal image&#x27;) ax1 = plt.subplot2grid((2, 2), (0, 1)) ax1.imshow(equ[:, :, ::-1]) ax1.set_title(&#x27;equalization&#x27;) ax1 = plt.subplot2grid((2, 2), (1, 0), colspan=3, rowspan=1) ax1.imshow(h_stack_img[:, :, ::-1]) ax1.set_title(&#x27;horizational&#x27;) plt.tight_layout() plt.show()plt.figure()plt.title(&quot;Grayscale Histogram&quot;)plt.xlabel(&quot;Bins&quot;)plt.ylabel(&quot;# of Pixels&quot;)# 灰度直方图img = cv2.imread(&#x27;lena.jpg&#x27;)GrayHist(img)# 颜色直方图colorHist(img)# 多通道直方图channelsHist(img)# 灰度直方图均衡grayEqualize(img)# 彩色图像直方图均衡colorEqualize(img) 灰度直方图 颜色直方图 多通道直方图 灰度直方图均衡 彩色图像直方图均衡","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【四】图像滤波","slug":"2019-03-04-cv-image_filter","date":"2019-03-21T16:00:00.000Z","updated":"2022-03-25T13:51:07.339Z","comments":true,"path":"2019/03/22/2019-03-04-cv-image_filter/","link":"","permalink":"https://aicc-cn.github.io/2019/03/22/2019-03-04-cv-image_filter/","excerpt":"","text":"图像滤波基本原理 图像信息在采集过程中往往受到各种噪声源的干扰，这些噪声在图像上的常常表现为一些孤立像素点，这可理解为像素的灰度是空间相关的，即噪声点像素灰度与它们临近像素的灰度有着显著不同。通常，一般的前置图像处理后的信息仍然带有后续所不希望夹带的孤立像素点，这种干扰或孤立像素点如不经过滤波处理，会对以后的图像区域分割、分析和判断带来影响。 基本图像预处理滤波方法图像滤波与卷积【公式定义】 与1维信号滤波类似，图像滤波由卷积定义。 在图像中， 以模板形式定义。 注意：如果滤波器是对称的，那么两个公式就是等价的。 ​ $f(x, y)$为图片原始数据、$g(x, y)$为卷积核。 【计算方式】举个栗子： 假设一张图像有 5x5 个像素，1 代表白，0 代表黑，这幅图像被视为 5x5 的单色图像。现在用一个由随机地 0 和 1 组成的 3x3 矩阵去和图像中的子区域做Hadamard乘积，每次迭代移动一个像素，这样该乘法会得到一个新的 3x3 的矩阵。下面的动图展示了这个过程。 直观上来理解： 用一个小的权重矩阵去覆盖输入数据，对应位置元素加权相乘，其和作为结果的一个像素点。 这个权重在输入数据上滑动，形成一张新的矩阵 这个权重矩阵就被称为卷积核（convolution kernel） 其覆盖的位置称为感受野（receptive fileld ） 生成的新矩阵叫做特征图（feature map） 分解开来，就如下图所示： 其中： 滑动的像素数量就叫做步长（stride），步长为1，表示跳过1个像素，步长为2，就表示跳过2个像素，以此类推 以卷积核的边还是中心点作为开始&#x2F;结束的依据，决定了卷积的补齐（padding）方式。前面我们所举的栗子是valid方式，而same方式则会在图像的边缘用0补齐，如将前面的valid改为same方式，如图所示： 其采样方式对应变换为： 我们前面所提到的输入图像都是灰色的，只有一个通道，但是我们一般会遇到输入通道不只有一个，那么卷积核是三阶的，也就是说所有的通道的结果做累加。 当然，最后，这里有一个术语：“偏置（bias）”，每个输出滤波器都有一个偏置项，偏置被添加到输出通道产生最终输出通道。 图像去噪【平均滤波】 在一个小区域内（通常是3*3）像素值平均。 数学公式定义 说明：如果在3*3的一个区域内，如果存在5个有效像素值，那么整体就除以5，以此类推。 【加权平均滤波】 在一个小区域内（通常是3*3）像素值加权平均。 数学公式定义 在进行基本平均滤波前，先把原始数据乘以一个指定的权重值，然后再进行平均。 普通卷积核 高斯卷积核 权重的分布符合高斯分布，所以叫高斯卷积核。 【中值滤波】 将滤波模板（含有若干个点的滑动窗口）在图像中漫游，并将模板中心与图中某个像素位置重合 读取模板中各对应像素的灰度值； 将这些灰度值从小到大排列； 取这一列数据的中间数据，将其赋给对应模板中心位置的像素。如果窗口中有奇数个元素，中值取元素按灰度值大小排序后的中间元素灰度值。如果窗口中有偶数个元素，中值取元素按灰度值大小排序后，中间两个元素灰度的平均值。因为图像为二维信号，中值滤波的窗口形状和尺寸对滤波器效果影响很大，不同图像内容和不同应用要求往往选用不同的窗口形状和尺寸。 示例 常用选取形式 3*3 5*5 中值滤波对椒盐噪声有效。 数学形态学滤波【概述】 形态学（morphology）一词通常表示生物学的一个分支，该分支主要研究动植物的形态和结构。而我们图像处理中指的形态学，往往表示的是数学形态学。 数学形态学（Mathematical morphology） 是一门建立在格论和拓扑学基础之上的图像分析学科，是数学形态学图像处理的基本理论。其基本的运算包括：二值腐蚀和膨胀、二值开闭运算、骨架抽取、极限腐蚀、击中击不中变换、形态学梯度、Top-hat变换、颗粒分析、流域变换、灰值腐蚀和膨胀、灰值开闭运算、灰值形态学梯度等。 【膨胀和腐蚀】 按数学方面来说，膨胀或者腐蚀操作就是将图像（或图像的一部分区域，我们称之为A）与核（我们称之为B）进行卷积。 核可以是任何的形状和大小，它拥有一个单独定义出来的参考点，我们称其为锚点（anchorpoint）。多数情况下，核是一个小的中间带有参考点和实心正方形或者圆盘，其实，我们可以把核视为模板或者掩码。 膨胀 膨胀就是求局部最大值的操作，核B与图形卷积，即计算核B覆盖的区域的像素点的最大值，并把这个最大值赋值给参考点指定的像素。这样就会使图像中的高亮区域逐渐增长。 数学定义 $A\\oplus B$表示集合A用结构元素B膨胀。 示例 腐蚀 腐蚀就是膨胀的相反操作，腐蚀就是求局部最小值的操作。 数学定义 $A\\ominus B$表示集合A用结构元素B腐蚀。 示例 【开闭运算】 膨胀和腐蚀并不互为逆运算，二者级联使用可生成新的形态学运算。 开运算 先腐蚀后膨胀 闭运算 先膨胀后腐蚀 先开后闭 可有效的去除噪声。 滤波总结 滤波即卷积，由逐点乘积后累加得到 平滑滤波包括平均滤波、高斯滤波、中值滤波等方法，其中高斯滤波最为常用 数学形态学滤波基于腐蚀与膨胀两个基本操作 代码演示123456789101112131415161718192021222324252627282930import cv2# 加载图片image = cv2.imread(&#x27;lena.jpg&#x27;)img = cv2.pyrDown(image)cv2.imshow(&quot;source image&quot;, img)# 高斯平滑滤波gaussImage = cv2.GaussianBlur(img, (5, 5), 0)cv2.imshow(&quot;gauss image&quot;, gaussImage)# 中值滤波medianImage = cv2.medianBlur(img, 5)cv2.imshow(&quot;median image&quot;, medianImage)# 形态学滤波kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))# 腐蚀图像eroded = cv2.erode(img, kernel)cv2.imshow(&quot;Eroded Image&quot;, eroded)# 膨胀图像dilated = cv2.dilate(img, kernel)cv2.imshow(&quot;dilated Image&quot;, dilated)# 形态学滤波-开运算morphImage_open = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)cv2.imshow(&quot;image-open&quot;, morphImage_open)# 形态学滤波-闭运算morphImage_close = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)cv2.imshow(&quot;image-close&quot;, morphImage_close)# 形态学滤波-先开后闭运算morphImage_open_close = cv2.morphologyEx(morphImage_open, cv2.MORPH_CLOSE, kernel)cv2.imshow(&quot;image-open-close&quot;, morphImage_open_close)cv2.waitKey(0)","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【七】图像特征描述","slug":"2019-03-07-cv-image_description","date":"2019-03-21T16:00:00.000Z","updated":"2022-03-25T13:51:21.324Z","comments":true,"path":"2019/03/22/2019-03-07-cv-image_description/","link":"","permalink":"https://aicc-cn.github.io/2019/03/22/2019-03-07-cv-image_description/","excerpt":"","text":"简单描述【简单描述符】 区域面积：区域包含的像素数 区域重心： 注意：区域重心可能不是整数。 【形状描述符】 形状参数 注意：形状为圆形时：F&#x3D;1；形状为其他时，F&gt;1 偏心率：等效椭圆宽高比。 欧拉数：$E &#x3D; C-H$ 圆形性： 一般化描述 最小包围矩形（MER） 方向和离心率 不变矩 首先定义归一化的中心矩 图像f(x,y)的p+q阶矩定义为： f(x,y)的p+q阶中心矩定义为： 其中： 即前面定义的重心。 f(x,y)的归一化中心矩定义为： 然后定义不变矩 常用的有七个不变矩，即对平移、旋转和尺度变化保持不变。这些可由归一化的二阶和三阶中心矩得到： 计算示例 图像特征描述总结 区域的基本特征包括面积、质心、圆形性等. 最小包围矩形和多边形拟合能有效的描述区域形状 七个不变矩特征可以有效的度量区域形状，对平移、旋转和比例放缩变换不敏感 代码实例1234567891011121314151617181920212223242526272829303132333435363738394041#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/1/16 16:39# @Author : Seven# @File : featureDescription.py# @Software: PyCharmimport cv2# 加载图片img = cv2.imread(&#x27;rice.png&#x27;)cv2.imshow(&#x27;source&#x27;, img)# 转换为灰度图grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# 形态学处理--五次开运算kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))morphImage_open = cv2.morphologyEx(grayImage, cv2.MORPH_OPEN, kernel, iterations=5)# 原图减去背景图riceImage = grayImage - morphImage_open# 大津算法阈值化thresholdImage = cv2.threshold(riceImage, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)# 图像分割partition = cv2.findContours(thresholdImage[1], cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)count = 0for i in partition[1]: area = cv2.contourArea(i) # print(&quot;area:&quot;, area) if area &lt;= 10: continue else: count += 1 print(&quot;blob:&#123;&#125;:&#123;&#125;&quot;.format(count, area)) # 得到米粒的坐标--x、y、w、h rect = cv2.boundingRect(i) # 在img中画出最大面积米粒 cv2.drawContours(img, [i], -1, (255, 255, 0), 1) cv2.rectangle(img, (rect[0], rect[1]), (rect[0]+rect[2], rect[1]+rect[3]), (0, 0, 255), 1) cv2.putText(img, str(count), (rect[0], rect[1]), cv2.FONT_HERSHEY_PLAIN, 0.5, (0, 255, 0))print(&quot;米粒数量：%s&quot; % count)cv2.imshow(&quot;area&quot;, img)cv2.waitKey(0)","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【九】直线检测","slug":"2019-03-09-cv-image_hough","date":"2019-03-21T16:00:00.000Z","updated":"2022-03-25T13:51:33.474Z","comments":true,"path":"2019/03/22/2019-03-09-cv-image_hough/","link":"","permalink":"https://aicc-cn.github.io/2019/03/22/2019-03-09-cv-image_hough/","excerpt":"","text":"Hough变换【简介】 Hough变换是图像处理中从图像中识别几何形状的基本方法之一。Hough变换的基本原理在于利用点与线的对偶性，将原始图像空间的给定的曲线通过曲线表达形式变为参数空间的一个点。这样就把原始图像中给定曲线的检测问题转化为寻找参数空间中的峰值问题。也即把检测整体特性转化为检测局部特性。比如直线、椭圆、圆、弧线等。 【原理】 采用参数空间变换的方法，对噪声和不间断直线的检测具有鲁棒性。 可用于检测圆和其他参数形状。 核心思想：直线$y &#x3D; kx +b $, 每一条直线对应一个k,b，极坐标下对应一个点($\\rho, \\theta$). 【直线】 直角坐标系的一点$(x, y)$，对应极坐标系下的一条正弦曲线$\\rho&#x3D; x cos\\theta+y\\sin\\theta$ 同一条直线上的多个点，在极坐标下必相交于一点。 【步骤】 将($\\rho, \\theta$)空间量化为许多小格。 根据$x-y$平面每一个直线点代入$\\theta$的量化值，算出各个$\\rho$，将对应格计数累加。 当全部点变换后，对小格进行检验。设置累计阈值T，计数器大于T的小格对应于共线点，其可以用作直线拟合参数。小于T的反映非共线点，丢弃不用。 注意：$\\theta$的取值为$(0,2\\pi)$. 【总结】 Hough变换的核心思想基于参数空间下的直线模型及投票原理。 Hough变换通过特征值判断灰度是否发生突变。 Hough变换想法直接，易于实现，是其它角点提取算法的基础。 代码实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/1/18 14:39# @Author : Seven# @File : HoughDemo.py# @Software: PyCharmimport cv2import numpy as npimg = cv2.imread(&#x27;sudoku.png&#x27;)edgeImage = cv2.Canny(img, 50, 200, apertureSize=3)grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)def basicHough(edgeImage): &quot;&quot;&quot; HoughLines(image, rho, theta, threshold, lines=None, srn=None, stn=None, min_theta=None, max_theta=None) image:边缘检测的输出图像. 它应该是个灰度图 (但事实上是个二值化图) lines:储存着检测到的直线的参数对 的容器 rho:参数极径 以像素值为单位的分辨率. 我们使用 1 像素. theta:参数极角 以弧度为单位的分辨率. 我们使用 1度 (即CV_PI/180) theta:要”检测” 一条直线所需最少的的曲线交点 srn and stn: 参数默认为0. :param img: :return: &quot;&quot;&quot; cv2.imshow(&#x27;edge&#x27;, edgeImage) lines = cv2.HoughLines(edgeImage, 1, np.pi / 180, 150) for rho, theta in lines[:, 0, :]: a = np.cos(theta) b = np.sin(theta) x0 = a * rho y0 = b * rho x1 = int(x0 + 1000 * (-b)) y1 = int(y0 + 1000 * a) x2 = int(x0 - 1000 * (-b)) y2 = int(y0 - 1000 * a) cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 2) cv2.imshow(&quot;hough&quot;, img)def uphough(edgeImage): &quot;&quot;&quot; HoughLinesP(image, rho, theta, threshold, lines=None, minLineLength=None, maxLineGap=None) image： 必须是二值图像，推荐使用canny边缘检测的结果图像； rho: 线段以像素为单位的距离精度，double类型的，推荐用1.0 theta： 线段以弧度为单位的角度精度，推荐用numpy.pi/180 threshod: 累加平面的阈值参数，int类型，超过设定阈值才被检测出线段，值越大，基本上意味着检出的线段越长，检出的线段个数越少。根据情况推荐先用100试试 lines：这个参数的意义未知，发现不同的lines对结果没影响，但是不要忽略了它的存在 minLineLength：线段以像素为单位的最小长度，根据应用场景设置 maxLineGap：同一方向上两条线段判定为一条线段的最大允许间隔（断裂），超过了设定值，则把两条线段当成一条线段，值越大，允许线段上的断裂越大，越有可能检出潜在的直线段 :param img: :return: &quot;&quot;&quot; cv2.imshow(&#x27;ep&#x27;, edgeImage) linesP = cv2.HoughLinesP(edgeImage, 1, np.pi/180, threshold=100, maxLineGap=100, minLineLength=10) for x1, x2, y1, y2 in linesP[:, 0, :]: cv2.line(img, (x1, x2), (y1, y2), (0, 0, 255), 2, cv2.LINE_AA) cv2.imshow(&#x27;houghP&#x27;, img)basicHough(edgeImage)uphough(edgeImage)cv2.waitKey(0)","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【八】图像分割","slug":"2019-03-08-cv-image_split","date":"2019-03-21T16:00:00.000Z","updated":"2022-03-25T13:51:27.332Z","comments":true,"path":"2019/03/22/2019-03-08-cv-image_split/","link":"","permalink":"https://aicc-cn.github.io/2019/03/22/2019-03-08-cv-image_split/","excerpt":"","text":"灰度阈值分割 假设：图像中的目标区和背景区之间或者不同目标区之间，存在不同的灰度或平均灰度。 凡是灰度值包含于z的像素都变成某一灰度值，其他的变成另一个灰度值，则该图像就以z为界被分成两个区域。 如果&#x3D;1和&#x3D;0，分割后的图像为二值图像 确定最佳阈值，使背景和目标之间的差异最大。 大津（Otsu）算法 确定最佳阈值，使背景和目标之间的类间方差最大 (因为二者差异最大)。 算法实现：遍历灰度取值。 区域生长法分割实现步骤： 对图像顺序扫描!找到第1个还没有归属的像素, 设该像素为(x0, y0); 以(x0, y0)为中心, 考虑(x0, y0)的8邻域像素(x, y)，如果(x, y)满足生长准则, 将(x, y)与 (x0, y0)合并, 同时将(x, y)压入堆栈; 从堆栈中取出一个像素, 把它当作(x0, y0)返回到步骤2; 当堆栈为空时，返回到步骤1; 重复步骤1 - 4直到图像中的每个点都有归属时。生长结束。 【总结】 【基于阈值的分割】 基于阈值的分割算法是最简单直接的分割算法。由于图像中目标位置和其他区域之间具有不同的灰度值，具有这种性质的目标区域通过阈值分割能够取得非常好的效果。通过阈值进行分割通常可以需要一个或多个灰度值作为阈值使图像分成不同的目标区域与背景区域。 如何找到合适的阈值进行分割是基于阈值的分割算法中最核心的问题。学者们针对这一问题进行了深入的研究。大津法（OTSU）、最大熵法等算法是其中比较突出的算法，这类算法在图像中适用固定的阈值。但是也有一类算法使用局部阈值，这类算法称为自适应阈值算法。这类算法根据图像的局部特征计算局部的阈值，通常情况下这类算法在有阴影或者图像灰度不均匀的情况下，具有比全局阈值更加好的分割效果。由于基于阈值的分割算法对噪声敏感，通常情况图像在分割之前需要进行图像去噪的操作。 OTSU算法通过计算前景和背景的两个类的最大类间方差得到阈值，阈值将整张图像分为前景和背景两个部分。 【基于区域的分割】 ​ 基于区域生长的分割算法将具有相似特征的像素集合聚集构成一个区域，这个区域中的相邻像素之间具有相似的性质。算法首先在每个区域中寻找一个像素点作为种子点，然后人工设定合适的生长规则与停止规则，这些规则可以是灰度级别的特征、纹理级别的特征、梯度级别的特征等，生长规则可以根据实际需要具体设置。满足生长规则的像素点视为具有相似特征，将这些像素点划分到种子点所在区域中。将新的像素点作为种子点重复上面的步骤，直到所有的种子点都执行的一遍，生成的区域就是该种子点所在的区域。 ​ 区域生长法的优势是整个算法计算简单，对于区域内部较为平滑的连通目标能分割得到很好的结果，同时算法对噪声不那么敏感。而它的缺点也非常明显，需要人为选定合适的区域生长种子点，而且生长规则选取的不合适可能导致区域内有空洞，甚至在复杂的图像中使用区域生长算法可以导致欠分割或者过分割。最后，作为一种串行的算法，当目标区域较大时，目标分割的速度较慢。 【基于边缘的分割】 ​ 通过区域的边缘来实现图像的分割是图像分割中常见的一种算法。由于不同区域中通常具有结构突变或者不连续的地方，这些地方往往能够为图像分割提供了有效的依据。这些不连续或者结构突变的地方称为边缘。图像中不同区域通常具有明显的边缘，利用边缘信息能够很好的实现对不同区域的分割。 ​ 基于边缘的图像分割算法最重要的是边缘的检测。图像的边缘通常是图像颜色、灰度性质不连续的位置。对于图像边缘的检测，通常使用边缘检测算子计算得出。常用的图像边缘检测算子有：Laplace算子、Sobel算子、Canny算子等。 代码演示123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899import cv2import matplotlib.pyplot as pltimport numpy as npimport random# 加载图片histImage = cv2.imread(&#x27;lena.jpg&#x27;)image_1 = cv2.imread(&#x27;pic2.png&#x27;)image_2 = cv2.imread(&#x27;pic6.png&#x27;)cv2.imshow(&quot;source-1&quot;, image_1)cv2.imshow(&quot;source-2&quot;, image_2)def GrayHist(img): &quot;&quot;&quot; calcHist(images, channels, mask, histSize, ranges, hist=None, accumulate=None) images — 源矩阵指针（输入图像，可以多张）。 但都应当有同样的深度（depth）, 比如CV_8U 或者 CV_32F ， 和同样的大小。每张图片可以拥有任意数量的通道。 channels — 通道的数量，每个通道都有编号，灰度图为一通道，即channel[1]=0;对应着一维。 BGR三通道图像编号为channels[3]=&#123;0,1,2&#125;;分别对应着第一维，第二维，第三维。 mask — 掩码图像：要对图像处理时，先看看这个像素对应的掩码位是否为屏蔽，如果为屏蔽，就是说该像素不处理（掩码值为0的像素都将被忽略） hist — 返回的直方图。 histSize — 直方图每一维的条目个数的数组，灰度图一维有256个条目。 ranges — 每一维的像素值的范围，传递数组，与维数相对应，灰度图一维像素值范围为0~255。 :param name: 命名标志 :param img: 图片数据 :return: &quot;&quot;&quot; grayHist = cv2.calcHist([img], [0], None, [256], [0.0, 255.0]) print(grayHist) plt.plot(grayHist) plt.show()def ThresholdImage(image, name): &quot;&quot;&quot; threshold(src, thresh, maxval, type, dst=None) src： 输入图，只能输入单通道图像，通常来说为灰度图 dst： 输出图 thresh： 阈值 maxval： 当像素值超过了阈值（或者小于阈值，根据type来决定），所赋予的值 type：二值化操作的类型，包含以下5种类型： cv2.THRESH_BINARY； cv2.THRESH_BINARY_INV； cv2.THRESH_TRUNC； cv2.THRESH_TOZERO；cv2.THRESH_TOZERO_INV :param img: :return: &quot;&quot;&quot; img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) listType = [cv2.THRESH_BINARY, cv2.THRESH_BINARY_INV, cv2.THRESH_TRUNC, cv2.THRESH_TOZERO, cv2.THRESH_TOZERO_INV] for i in listType: thresholdImage = cv2.threshold(img, 125, 255, i) cv2.imshow(&quot;threshold-%s-%s&quot; % (name, i), thresholdImage[1])def floodFillImage(img, name, lo, up): &quot;&quot;&quot; floodFill(image, mask, seedPoint, newVal, loDiff=None, upDiff=None, flags=None) image 【输入/输出】 1或者3通道、 8bit或者浮点图像。仅当参数flags的FLOODFILL_MASK_ONLY标志位被设置时image不会被修改，否则会被修改。 mask 【输入/输出】 操作掩码，必须为单通道、8bit，且比image宽2个像素、高2个像素。使用前必须先初始化。 Flood-filling无法跨越mask中的非0像素。例如，一个边缘检测的结果可以作为mask来阻止边缘填充。 在输出中，mask中与image中填充像素对应的像素点被设置为1，或者flags标志位中设置的值(详见flags标志位的解释)。 此外，该函数还用1填充了mask的边缘来简化内部处理。因此，可以在多个调用中使用同一mask，以确保填充区域不会重叠。 seedPoint 起始像素点 newVal 重绘像素区域的新的填充值(颜色) loDiff 当前选定像素与其连通区中相邻像素中的一个像素，或者与加入该连通区的一个seedPoint像素，二者之间的最大下行差异值。 upDiff 当前选定像素与其连通区中相邻像素中的一个像素，或者与加入该连通区的一个seedPoint像素，二者之间的最大上行差异值。 flags flags标志位是一个32bit的int类型数据，其由3部分组成： 0-7bit表示邻接性(4邻接、8邻接)； 8-15bit表示mask的填充颜色；16-31bit表示填充模式（详见填充模式解释） :param img: :param name: :return: &quot;&quot;&quot; seed = (10, 30) # 起始位置 # 构建mask,根据mask参数的介绍,其size必须为宽img+2,高img+2 mask = np.zeros((img.shape[0] + 2, img.shape[1] + 2), dtype=np.uint8) newVal = (255, 64, 64) # img fill的填充颜色值 # 执行floodFill操作 # ret, image, mask, rect = cv2.floodFill(img, mask, seed, newVal) cv2.floodFill(img, mask, seed, newVal, (lo, )*3, (up, )*3, cv2.FLOODFILL_FIXED_RANGE) cv2.imshow(&#x27;floodfill-%s&#x27; % name, img)# 计算并显示直方图GrayHist(image_1)# 大津算法ThresholdImage(image_1, &#x27;1&#x27;)ThresholdImage(image_2, &#x27;2&#x27;)# riceImage = cv2.imread(&#x27;rice.png&#x27;)# ThresholdImage(riceImage, &#x27;rice&#x27;)floodFillImage(image_1, &#x27;1&#x27;, 200, 255)floodFillImage(image_2, &#x27;2&#x27;, 0, 0)cv2.waitKey(0)","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【十一】SIFT角点检测","slug":"2019-03-11-cv-image_sift","date":"2019-03-21T16:00:00.000Z","updated":"2022-03-25T13:51:45.626Z","comments":true,"path":"2019/03/22/2019-03-11-cv-image_sift/","link":"","permalink":"https://aicc-cn.github.io/2019/03/22/2019-03-11-cv-image_sift/","excerpt":"","text":"概述 SIFT的全称是Scale Invariant Feature Transform（尺度不变特征变换），是由加拿大教授David G.Lowe在1999年在会议文章中提出，2004年发表在IJCV上。 是计算机视觉界近二十年来引用率最高的文章之一。 SIFT特征对旋转、尺度缩放、亮度变化等保持不发性，是一种稳定的局部特征。 SIFT的特征提取方面对计算机视觉近年来的发展影响深远，特别是几乎影响到了后续所有的角点提取和匹配算法。 算法特点 图像的局部特征，对旋转、尺度缩放、亮度变化保持不变，对视角变化、仿射变换、噪声也保持一定程度的稳定性。 独特性好，信息量丰富，适用于海量特征库进行快速、准确的匹配。 多量性：即使是很少几个物体也可以产生大量的SIFT特征 高速性：改进的SIFT匹配算法甚至可以达到实时性 扩展性：可以很方便的与其他的特征向量进行联合。 尺度空间概述 人眼可自动调节尺度，完成对物体的检测和识别 模仿人的视觉认知，把物体不同尺度下的图像都提供给机器，让机器能够对物体在不同的尺度下综合信息识别。 因此，首先需要建立尺度空间。 通过高斯函数与原图像卷积，并经过下采样，可建立原始图像的尺度空间模型。 理论 在图像信息处理模型中引入一个被视为尺度的参数，通过连续变化尺度参数获得多尺度下的尺度空间表示序列，对这些序列进行尺度空间主轮廓的提取，并以该主轮廓作为一种特征向量，实现边缘、角点检测和不同分辨率上的特征提取等。 尺度空间方法将传统的单尺度图像信息处理技术纳入尺度不断变化的动态分析框架中，更容易获取图像的本质特征。尺度空间中各尺度图像的模糊程度逐渐发大，能够模拟人在距离目标由近到远时目标在视网膜上的形成过程。 定义 高斯卷积核是实现尺度变换的唯一变换核 一幅图像的尺度空间被定义为：高斯卷积核与该图像的卷积，它是高斯卷积核中的参数$\\sigma$的函数，这里用到“空间”这个词，是因为$\\sigma$是可以连续变化的，具体地，图像$I(x,y)$的尺度空间为： $L(x,y,\\sigma) &#x3D; G(x,y,\\sigma)*I(x,y)$ $G(x,y,\\sigma) &#x3D; \\frac{1}{2\\pi\\sigma^2}*e^{-\\frac{x^2+y^2}{2\\sigma^2}}$ 是高斯核函数。 $\\sigma$是尺度因子，$\\sigma$的大小决定图像的平滑程度，大尺度对应图像的概貌特征，小尺度对应图像的细节特征。大的$\\sigma$值对应粗糙尺度(低分辨率)，小的$\\sigma$值对应精细尺度(高分辨率)。 *是卷积操作。 $L(x,y,\\sigma) $就是输入图像$I(x,y)$的尺度空间。 栗子 ​ 比如要观察一棵树，所选择的尺度应该是“米”级；如果观察树上的树叶，所选择的尺度则应该是“厘米”级。一般来说，摄像设备所能呈现的画面分辨率是固定的。要以同样的分辨率分别观察树和树叶，我们需要调整摄像设备的摄像位置。因此，视觉问题中的“尺度”概念也可以被形象地理解为摄像设备与被观察物体之间的距离：较远的距离对应较大的尺度，较近的距离对应较小的尺度。 概况的说：“尺度空间”的概念就是在多个尺度下观察目标，然后加以综合的分析和理解。 特征提取一幅图像的SIFT特征提取，分为4个步骤： 尺度空间极值检测. 关键点位置及尺度确定 关键点方向确定 特征向量生成 尺度空间极值检测 SIFT特征点其实就是尺度空间中稳定的点&#x2F;极值点，那么，为了得到这些稳定点： 首先，需要对输入图像建立尺度空间（通过图像高斯金字塔） 然后，需要从建立得到的尺度空间中检测极值点（转换为图像差分高斯金字塔中极值点的检测） 获取差分高斯金字塔 DoG中极值点的检测 【构建高斯金字塔】 尺度空间在实现时使用高斯金字塔表示，高斯金字塔的构建分为两部分： 对图像做不同尺度的高斯模糊； 对图像做降采样(隔点采样)。 图像的金字塔模型是指，将原始图像不断降阶采样，得到一系列大小不一的图像，由大到小，从下到上构成的塔状模型。 ​ 对于一幅输入图像，为了进行sift特征检测、实现scale-invariant（任何尺度下都能够有对应的特征点），需要对该图像的尺度空间进行分析，即建立高斯金字塔图像、得到不同scale的图像，这里的高斯金字塔与最原始的高斯金字塔稍微有点区别，因为它在构造尺度空间时，将这些不同尺度图像分为了多个Octave、每个Octave又分为了多层。下图左侧框内给出了Sift中的高斯金字塔的结构图。 高斯金字塔分为S组（即S个塔、或者称为S个octave），每个塔中有s层；具体有几个塔、每个塔包含几层由输入图像的尺寸决定！对于上图而言：有S&#x3D;2组，每组由s&#x3D;5层； 同一塔中图像尺寸相同但尺度不同、不同塔中图像尺寸和尺度都不同（注意：这里的尺寸和尺度概念不同！尺寸对应图像分辨率、尺度为高斯核大小） 每一个塔中图像尺寸确定方法为：第一个塔的尺寸为原图大小，后面每个塔的尺寸为上一个塔的降采样的结果（即长宽分别减半的结果） 每个塔中各层图像的尺度不同，即$\\sigma$取值不同，各层之间相差一个k值。例如：上图中第一个塔中各层的尺度因子分别为$\\sigma、k\\sigma、 k^2\\sigma、k^3\\sigma、k^4\\sigma$；而第二个塔中各层的尺度因子分别为$2\\sigma、2k\\sigma、 2k^2\\sigma、2k^3\\sigma、2k^4\\sigma$ 如何确定相邻层之间的比例因子k呢？下图是同一Octave中不同层和不同Octave之间的尺度大小关系，为了满足尺度变化的连续性，即某一组的最后一层对应的尺度与下一组的第一层对应的尺度应该相差一个k倍，所以应该有**$k^{s-1}\\sigma_0 k &#x3D; 2\\sigma_0$， 所以$k^s&#x3D;2$，即$k&#x3D;2^{1&#x2F;2}$， 其中，$\\sigma_0$**为基础尺度因子。 注意：关于尺度变化的连续性应该就是指：金字塔图像中所有图像之间的尺度因子都相差一个k倍：每个octave中相邻图像之间尺度因子相差一个k，相邻2个octave的最后一层与第一层的图像之间尺度因子相差一个k！注2：英文Octave是音乐上一个八度的意思，在这里指的是一组图像。这一组图像的分辨率是相同的，但是采用了不同尺度的高斯函数进行滤波，因此从模糊程度上看（或者说从关注的尺度上看）是有区别的。而不同组的图像具有不同的分辨率，在尺度上的差别就更大。 ​ 得到了图像的尺度空间后，需要在尺度中间内检测稳定特征点，从而需要比较不同尺度图像之间的差别，实现极值点的检测，实际上，Laplacian of Gaussian和Difference of Gaussian都能够实现这样的目的，但LoG需要大量的计算，而DoG的计算相对简单。 【建立图像差分高斯金字塔（DoG）】​ 对于高斯金字塔中的每一个塔的不同层，可以计算得到相邻层之间的差值，从而可以得到差分高斯，对高斯金字塔中每一个塔都进行同样的操作，从而得到差分高斯金字塔，如下图右侧所示，即显示了由左侧的高斯金字塔构造得到的差分高斯金字塔，该差分高斯金字塔包含2个塔，每个塔都有四层 差分高斯表征了相邻尺度的高斯图像之前的差别，大值表示区别大，小值表示区别小，后续的特征点检测都是差分高斯图像金字塔中进行的！ 【尺度空间中特征点的检测（DoG中极值点的检测）】 构造完尺度空间（差分高斯金字塔）后，接下来的任务就是“在尺度中间中检测出图像中的稳定特征点”. ​ 对于DoG中每一个采样点（每一个Octave中每一层），将其与它邻域内所有像素点（8+18&#x3D;26）进行比较，判断其是否为局部极值点（极大或者极小），更加具体地：如下图所示，中间的检测点和它同尺度的8个相邻点和上下相邻尺度对应的9×2个点共26个点比较，以确保在尺度空间和二维图像空间都检测到极值点。 一个点如果在DOG尺度空间本层以及上下两层的26个领域中是最大或最小值时，就认为该点是图像在该尺度下的一个特征点。但要注意：这种相邻层之间的极值点的寻找是在同一Octave中的相邻尺度之间进行寻找的，而不要跨组！ 同时，应该注意到一个问题，在极值比较的过程中，每一Octave图像的首末两层是无法进行极值比较的，为了满足尺度变化的连续性，需要进行一些修正：在高斯金字塔中生成S+3层图像，具体解释如下：假设s&#x3D;3，也就是每个塔里有3层，则k&#x3D;21&#x2F;s&#x3D;21&#x2F;3： 那么按照上图可得Gauss Space和DoG space 分别有3个（s个）和2个（s-1个）分量，在DoG space中，1st-octave两项分别是σ,kσ; 2nd-octave两项分别是2σ,2kσ; 由于无法比较极值，我们必须在高斯空间继续添加高斯模糊项，使得DoG中第一个Octave形成σ,kσ,k2σ,k3σ,k4σ，这样就可以选择中间三项kσ,k2σ,k3σ（只有左右都有才能有极值）；那么下一octave中（由上一层降采样获得）所得三项即为2kσ,2k2σ,2k3σ，其首项2kσ&#x3D;24&#x2F;3。刚好与上一octave末项k3σ&#x3D;23&#x2F;3尺度变化连续起来，所以每次要在Gaussian space添加3项，每组（塔）共S+3层图像，相应的DoG金字塔有S+2层图像。 关键点位置及尺度确定​ 通过拟和“三维二次函数”可以精确确定关键点的位置和尺度（达到亚像素精度），具体方法还未知，可以得到一系列的SIFT候选特征点集合，但由于这些关键点中有些具有较低的对比对，有些输属于不稳定的边缘响应点(因为DoG算子会产生较强的边缘响应)，所以，为了增强匹配稳定性、提高抗噪声能力，应该将这2类关键点去除，实现对候选SIFT特征点集合的进一步净化： 剔除对比度低的点 剔除边缘点 关键点方向确定 算关键点的方向，需要利用以该关键点为中心的某邻域窗口内所有像素点的梯度分布特性（目的是为了使的sift算子具备旋转不变性），所以，下面首先给出计算尺度空间中每个像素点的梯度模值和方向的方法，按照下面公式计算 其中L所用的尺度为每个关键点各自所在的尺度。 接下来，对于每个关键点（假设尺度为sigma），利用直方图统计其相邻窗口内的像素的梯度分布，从而，确定该关键点的方向，具体方法如下： 分别计算以该关键点为中心的相邻窗口中像素点的梯度方向和模值. 为该窗口内每一个像素点赋予一个权值：由每个像素点对应的梯度模值和以关键点为中心尺度为1.5sigma的高斯核决定 设定某种规则，开始统计直方图，例如，这里将方向（0~360°）分为8份，那么，梯度方向直方图将有8个柱，窗口内每个像素到底属于哪个柱由该像素点的梯度方向确定（见下图右侧子图所示） 在该关键点对应的梯度方向直方图中，找到包含像素点最多的那个方向作为该关键点的方向，对于上图而言，方向角(0.25π,0.5π)这个区间内的像素点最多，所以，以(0.25π+0.5π)&#x2F;2 &#x3D; 0.375π作为该关键点的方向 。 至此，得到了图像中所有关键点的方向！实际上，关键点方向的确定是为了接下来的特征向量生成中的坐标旋转使用的！ 特征向量生成上面只是得到了每个关键点的方向，接下来，需要确定每个关键点的特征向量，具体方式如下： 将坐标轴旋转到关键点的方向 对于某一个关键点，取其周围窗口，如下图绿色区域所示，其中，窗口内每个小矩形框为一个像素，箭头表示该像素位置的梯度方向和模值 在该$88$窗口对应的4个（称为4个种子点）$44$的小块儿上，分别计算该小块儿包含的16个像素点的梯度直方图（8个柱），并进行累加（每个柱对应的所有像素点的梯度模值累加），每个小块儿可以得到8个特征（8个方向对应的模值的累加），从而，4个种子点将得到关键点的4*8&#x3D;32个特征，如下图右侧所示，4个种子点，每个种子点产生8个特征 . 实际中，Lowe建议使用$44$个子块儿（称为16个种子点）进行特征计算，那么，每个关键点将具有16*8&#x3D;128个特征，如下图所示，此时，需要在特征点周围取$1616$的窗口，分割为$4*4$个子块儿（这样做的目的是为了增强后续匹配的稳健性）。 至此，关键点特征向量完全确定！此时SIFT特征向量已经去除了尺度变化、旋转等几何变形因素的影响，再继续将特征向量的长度归一化，则可以进一步去除光照变化的影响。 特征匹配现有A、B两幅图像，分别利用上面的方法从各幅图像中提取到了k1个sift特征点和k2个特征点及其对应的特征描述子，即$k1128$维和$k2128$维的特征，现在需要将两图中各个scale（所有scale）的描述子进行匹配。 接下来采用关键点特征向量的欧式距离来作为两幅图像中关键点的相似性判定度量。 取图像A中的某个关键点，并找出其与图像B中欧式距离最近的前两个关键点 在这两个关键点中，如果最近的距离除以次近的距离少于某个比例阈值，则接受这一对匹配点。降低这个比例阈值，SIFT匹配点数目会减少，但更加稳定。 利用2个近邻点比较的目的是为了排除因为图像遮挡和背景混乱而产生的无匹配关系的关键点，所以Lowe提出了比较最近邻距离与次近邻距离的方法,距离比率ratio小于某个阈值的认为是正确匹配。因为对于错误匹配,由于特征空间的高维性,相似的距离可能有大量其他的错误匹配,从而它的ratio值比较高。Lowe推荐ratio的阈值为0.8。但作者对大量任意存在尺度、旋转和亮度变化的两幅图片进行匹配，结果表明ratio取值在0. 4~0. 6之间最佳，小于0. 4的很少有匹配点，大于0. 6的则存在大量错误匹配点。(如果这个地方你要改进，最好给出一个匹配率和ration之间的关系图，这样才有说服力)，作者建议ratio的取值原则如下: ratio&#x3D;0. 4 对于准确度要求高的匹配； ratio&#x3D;0. 6 对于匹配点数目要求比较多的匹配； ratio&#x3D;0. 5 一般情况下。 也可按如下原则: 当最近邻距离&lt;200时，ratio&#x3D;0. 6；反之，ratio&#x3D;0. 4。ratio的取值策略能排分错误匹配点。 总结 尺度空间的核心思想是通过不同分辨率看同一个图像，可通过不同尺度的高斯函数不原始图像卷积实现 极大值点的精确定位可通过解含有二阶寻数项的方程得到 借助在关键点附近区域的梯度方向统计不直方图生成，SIFT最终可得到对应每个关键点的128维向量描述。 代码实例123456789101112131415161718192021222324252627#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/1/18 22:04# @Author : Seven# @File : surfDemo.py# @Software: PyCharmimport cv2import numpy as npimg1 = cv2.imread(&#x27;csdn.png&#x27;)img2 = np.rot90(img1)# 特征点检测和提取特征点描述子surf = cv2.xfeatures2d.SURF_create(50)kp1, des1 = surf.detectAndCompute(img1, None)kp2, des2 = surf.detectAndCompute(img2, None)# 创建 BFMatcher 对象bf = cv2.BFMatcher(cv2.NORM_L2)# 根据描述子匹配特征点.matches = bf.match(des1, des2)# 画出50个匹配点img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches[:50], None, flags=2)cv2.imshow(&quot;img&quot;, img3)cv2.waitKey(0)","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【十】Harris角点检测","slug":"2019-03-10-cv-image_harris","date":"2019-03-21T16:00:00.000Z","updated":"2022-03-25T13:51:38.787Z","comments":true,"path":"2019/03/22/2019-03-10-cv-image_harris/","link":"","permalink":"https://aicc-cn.github.io/2019/03/22/2019-03-10-cv-image_harris/","excerpt":"","text":"Harris角点检测【原理】 人眼对角点的识别通常是在一个局部的小区域或小窗口完成的。如果在各个方向上移动这个特征的小窗口，窗口内区域的灰度发生了较大的变化，那么就认为在窗口内遇到了角点。如果这个特定的窗口在图像各个方向上移动时，窗口内图像的灰度没有发生变化，那么窗口内就不存在角点；如果窗口在某一个方向移动时，窗口内图像的灰度发生了较大的变化，而在另一些方向上没有发生变化，那么，窗口内的图像可能就是一条直线的线段。 在灰度变化平缓区域，窗口内像素灰度积分近似保持不变 在 边缘区域，边缘方向：灰度积分近似不变， 其余任意方向：剧烈变化。 在角点出：任意方向剧烈变化。 【数学推导】 定义灰度积分变化 定义灰度积分变化 如果$u、v$很小， 则有： 其中 注意$E$是一个二次型，即： $\\lambda_1^{-\\frac12}、\\lambda_2^{-\\frac12}$是椭圆的长短轴 当$\\lambda_1、\\lambda_2$都比较小时，点(x, y)处于灰度变化平缓区域. 当$\\lambda_1&gt;\\lambda_2$或者$\\lambda_1&lt;\\lambda_2$时，点(x, y)为边界元素. 当$\\lambda_1、\\lambda_2$都比较大时，且近似相等，点(x, y)为角点. 角点响应函数 当R接近于零时，处于灰度变化平缓区域 当R&lt;0时，点为边界像素 当R&gt;0时，点为角点 代码实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/1/18 18:51# @Author : Seven# @File : HarrisDemo.py# @Software: PyCharmimport cv2import numpy as npimg = cv2.imread(&#x27;chess.png&#x27;)img = cv2.pyrDown(img)cv2.imshow(&#x27;source&#x27;, img)def harrisDemo(img): &quot;&quot;&quot; cornerHarris(src, blockSize, ksize, k, dst=None, borderType=None) src，输入图像，即源图像，填Mat类的对象即可，且需为单通道8位或者浮点型图像。 dst，函数调用后的运算结果存在这里，即这个参数用于存放Harris角点检测的输出结果，和源图片有一样的尺寸和类型。 blockSize，表示邻域的大小，更多的详细信息在cornerEigenValsAndVecs中有讲到。 ksize，表示Sobel()算子的孔径大小。 k，Harris 角点检测方程中的自由参数，取值参数为[0,04，0.06] borderType，图像像素的边界模式，注意它有默认值BORDER_DEFAULT。更详细的解释，参考borderInterpolate函数。 :param img: :return: &quot;&quot;&quot; grayImage = np.float32(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)) # 输入图像必须是float32，最后一个参数在0.04 到0.05 之间 # harris角点检测 harrisImg = cv2.cornerHarris(grayImage, 2, 3, 0.04) # 膨胀 dst = cv2.dilate(harrisImg, None) # 0.01是人为设定的阈值 # 把角点设为红色 img[dst &gt; 0.01 * dst.max()] = [0, 0, 255] cv2.imshow(&#x27;harris&#x27;, img)def harrisSubPix(img): &quot;&quot;&quot; cornerSubPix(image, corners, winSize, zeroZone, criteria) image：输入图像 corners：输入角点的初始坐标以及精准化后的坐标用于输出。 winSize：搜索窗口边长的一半，例如如果winSize=Size(5,5)，则一个大小为(5*2+1)*(5*2+1)=11*11的搜索窗口将被使用。 zeroZone：搜索区域中间的dead region边长的一半，有时用于避免自相关矩阵的奇异性。如果值设为(-1,-1)则表示没有这个区域。 criteria：角点精准化迭代过程的终止条件。也就是当迭代次数超过criteria.maxCount，或者角点位置变化小于criteria.epsilon时，停止迭代过程。 :param img: :return: &quot;&quot;&quot; gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # harris角点检测 gray = np.float32(gray) dst = cv2.cornerHarris(gray, 2, 3, 0.04) dst = cv2.dilate(dst, None) ret, dst = cv2.threshold(dst, 0.01 * dst.max(), 255, 0) dst = np.uint8(dst) # 获取质心坐标-centroids 是每个域的质心坐标 ret, labels, stats, centroids = cv2.connectedComponentsWithStats(dst) # 设置停止和转换条件 criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001) corners = cv2.cornerSubPix(gray, np.float32(centroids), (5, 5), (-1, -1), criteria) # Now draw them res = np.hstack((centroids, corners)) res = np.int0(res) img[res[:, 1], res[:, 0]] = [0, 0, 255] img[res[:, 3], res[:, 2]] = [0, 255, 0] cv2.imshow(&#x27;SubPix&#x27;, img)def harrisImage(img): grayImage = np.float32(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)) # 输入图像必须是float32，最后一个参数在0.04 到0.05 之间 # harris角点检测 harrisImg = cv2.cornerHarris(grayImage, 2, 3, 0.04) # 膨胀 dst = cv2.dilate(harrisImg, None) # 对角点图像进行阈值化 ret, dst = cv2.threshold(dst, 0.01 * dst.max(), 255, 0) # 转换为8位图像 dst = np.uint8(dst) # 获取质心坐标-centroids 是每个域的质心坐标 ret, labels, stats, centroids = cv2.connectedComponentsWithStats(dst) criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001) # 计算得出角点坐标 corners = cv2.cornerSubPix(grayImage, np.float32(centroids), (5, 5), (-1, -1), criteria) # 对所有角点用0圈出来 for point in corners: cv2.circle(img, tuple(point), 5, (0, 0, 255), 2) cv2.imshow(&#x27;harris-2&#x27;, img)harrisDemo(img)harrisSubPix(img)harrisImage(img)cv2.waitKey(0)","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"【十二】ORB角点检测","slug":"2019-03-12-cv-image_orb","date":"2019-03-21T16:00:00.000Z","updated":"2022-03-25T13:51:51.139Z","comments":true,"path":"2019/03/22/2019-03-12-cv-image_orb/","link":"","permalink":"https://aicc-cn.github.io/2019/03/22/2019-03-12-cv-image_orb/","excerpt":"","text":"ORB角点检测【简介】 ORB（Oriented FAST and Rotated BRIEF）是一种快速特征点提取和描述的算法。 这个算法是由Ethan Rublee, Vincent Rabaud, Kurt Konolige以及Gary R.Bradski在2011年一篇名为“ORB：An Efficient Alternative to SIFT or SURF”的文章中提出。 ORB算法分为两部分，分别是特征点提取和特征点描述。特征提取是由FAST（Features from Accelerated Segment Test）算法发展来的，特征点描述是根据BRIEF（Binary Robust Independent Elementary Features）特征描述算法改进的。 ORB &#x3D; oFast + rBRIEF。据称ORB算法的速度是sift的100倍，是surf的10倍。 ORB算子在SLAM及无人机视觉等领域得到广泛应用 【oFAST特征提取】 ORB算法的特征提取是由FAST算法改进的，这里称为oFAST（FAST keypoint Orientation）。 在使用FAST提取出特征点之后，给其定义一个特征点方向，以此来实现特征点的旋转不变性。 粗提取 判断特征点：从图像中选取一点P，以P为圆心画一个半径为3像素的圆。圆周上如果有连续N个像素点的灰度值比P点的灰度值大戒小，则认为P为特征点。这就是大家经常说的FAST-N。有FAST-9、FAST-10、FAST-11、FAST-12，大家使用比较多的是FAST-9和FAST-12。 快速算法：为了加快特征点的提取，快速排出非特征点，首先检测1、5、9、13位置上的灰度值，如果P是特征点，那么这四个位置上有3个或3个以上的的像素值都大于或者小于P点的灰度值。如果不满足，则直接排出此点。 筛选最优特征点机器学习的方法筛选最优特征点。简单来说就是使用ID3算法训练一个决策树，将特征点圆周上的16个像素输入决策树中，以此来筛选出最优的FAST特征点。具体步骤如下： 选取进行角点提取的应用场景下的一组训练图像。 使用FAST角点检测算法找出训练图像上的所有角点。 对于每一个角点，将其周围的16个像素存储成一个向量。对所有图像都这样做构建一个特征向量。 每一个角点的16像素点都属于下列三类中的一种，像素点因此被分成三个子集:$P_d、P_s、P_b$ 定义一个新的布尔变量Kp ，如果是角点就设置为True，否则就设置为False。 使用ID3算法来查询每一个子集。 递归计算所有的子集直到它的熵为0。 注意：被构建好的决策树可用于其它图像的FAST检测。 使用非极大值抑制算法去除临近位置多个特征点 计算特征点出的FAST得分值s（像素点不周围16个像素点差值的绝对值之和） 以特征点p为中心的一个邻域（如3x3或5x5）内，若有多个特征点，则判断每个特征点的s值 若p是邻域所有特征点中响应值最大的，则保留；否则，抑制。若邻域内只有一个特征点，则保留。得分计算公式如下（公式中用V表示得分，t表示阈值）： 建立金字塔以实现特征点多尺度不变性 设置一个比例因子scaleFactor（opencv默认为1.2）和金字塔的层数nlevels（Opencv默认为8）。 将原图像按比例因子缩小成nlevels幅图像。 缩放后的图像为：I’&#x3D; I&#x2F;scaleFactork(k&#x3D;1,2,…, nlevels)。nlevels幅不同比例的图像提取特征点总和作为这幅图像的oFAST特征点。 特征点的旋转不变性。ORB算法提出使用矩（moment）法来确定FAST特征点的方向。也就是说通过矩来计算特征点以r为半径范围内的质心，特征点坐标到质心形成一个向量作为该特征点的方向。矩定义如下:$m_{pq}&#x3D;\\Sigma_{x,y} x^p y^q I(x,y), x,y \\in [-r,r]$ 其中，I(x,y)为图像灰度表达式。该矩的质心为：$C &#x3D; ( \\dfrac{m_{10}}{m_{00}}, \\dfrac{m_{01}}{m_{00}})$ 假设角点坐标为O，则向量的角度即为该特征点的方向。计算公式如下：$\\theta &#x3D; arctan(m_{01}&#x2F;m_{10})$ 算法特点 FAST算法比其他角点检测算法要快 受图像噪声以及设定阈值影响较大 当设置n&lt;12n&lt;12时，不能用快速方法过滤非角点 FAST不产生多尺度特征，不具备旋转不变性，而且检测到的角点不是最优 【rBRIEF】 ORB算法的特征描述是由BRIEF算法改进的，这里称为rBRIEF（Rotation-Aware Brief）。也就是说，在BRIEF特征描述的基础上加入旋转因子从而改进BRIEF算法。 算法描述 得到特征点后我们需要以某种方式描述这些特征点的属性。这些属性的输出我们称之为该特征点的描述子（Feature Descritors）.ORB采用BRIEF算法来计算一个特征点的描述子。BRIEF算法的核心思想是在关键点P的周围以一定模式选取N个点对，把这N个点对的比较结果组合起来作为描述子。 BRIEF算法计算出来的是一个二进制串的特征描述符。它是在一个特征点的邻域内，选择n对像素点$p_i、q_i（i&#x3D;1,2,…,n）$。 比较每个点对的灰度值的大小，如果$I(p_i)&gt; I(q_i)$，则生成二进制串中的1，否则为0。 所有的点对都进行比较，则生成长度为n的二进制串。一般n取128、256戒512，opencv默认为256。 算法步骤 以关键点P为圆心，以d为半径做圆O。 在圆O内某一模式选取N个点对。这里为方便说明，N&#x3D;4，实际应用中N可以取512. 假设当前选取的4个点对如上图所示分别标记为： 定义T： 分别对已选取的点对进行T操作，将得到的结果进行组合。 则最终的描述子为：1011 具有旋转不变性的BRIEF​ ORB并没有解决尺度一致性问题，在OpenCV的ORB实现中采用了图像金字塔来改善这方面的性能。ORB主要解决BRIEF描述子不具备旋转不变性的问题。 steered BRIEF（旋转不变性改进） 在使用oFast算法计算出的特征点中包括了特征点的方向角度。假设原始的BRIEF算法在特征点SxS（一般S取31）邻域内选取n对点集。 经过旋转角度θ旋转，得到新的点对$D_\\theta&#x3D;R_\\theta D$ 在新的点集位置上比较点对的大小形成二进制串的描述符。 描述子的区分性 通过上述方法得到的特征描述子具有旋转不变性，称为steered BRIEF(sBRIEF)，但匹配效果却不如原始BRIEF算法，因为可区分性减弱了。特征描述子的一个要求就是要尽可能地表达特征点的独特性，便于区分不同的特征点。如下图所示，为几种特征描述子的均值分布，横轴为均值与0.5之间的距离，纵轴为相应均值下特征点的统计数量。可以看出，BRIEF描述子所有比特位的均值接近于0.5，且方差很大；方差越大表明可区分性越好。不同特征点的描述子表现出较大的差异性，不易造成无匹配。但steered BRIEF进行了坐标旋转，损失了这个特性，导致可区分性减弱，相关性变强，不利于匹配。 为了解决steered BRIEF可区分性降低的问题，ORB使用了一种基于学习的方法来选择一定数量的随机点对。 首先建立一个大约300k特征点的数据集(特征点来源于PASCAL2006中的图像)，对每个特征点，考虑其31×31的邻域Patch，为了去除噪声的干扰，选择5×5的子窗口的灰度均值代替单个像素的灰度，这样每个Patch内就有N&#x3D;(31−5+1)×(31−5+1)&#x3D;27×27&#x3D;729个子窗口，从中随机选取2个非重复的子窗口，一共有$M&#x3D;C_N^2$中方法。 这样，每个特征点便可提取出一个长度为M的二进制串，所有特征点可构成一个300k×M的二进制矩阵Q，矩阵中每个元素取值为0或1。现在需要从M个点对中选取256个相关性最小、可区分性最大的点对，作为最终的二进制编码。筛选方法如下： 对矩阵Q的每一列求取均值，并根据均值与0.5之间的距离从小到大的顺序，依次对所有列向量进行重新排序，得到矩阵T 将T中的第一列向量放到结果矩阵R中 取出T中的下一列向量，计算其与矩阵R中所有列向量的相关性，如果相关系数小于给定阈值，则将T中的该列向量移至矩阵R中，否则丢弃 循环执行上一步，直到R中有256个列向量；如果遍历T中所有列，R中向量列数还不满256，则增大阈值，重复以上步骤。 这样，最后得到的就是相关性最小的256对随机点，该方法称为rBRIEF。 【总结】 ORB &#x3D; oFAST + rBRIEF oFAST是一类快速角点检测算法，并具备旋转不变性 rBRIEF是一类角点描述(编码算法)，并且编码具有良好的可区分性 代码实例123456789101112131415161718192021222324252627#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/1/19 13:30# @Author : Seven# @File : ORBDemo.py# @Software: PyCharmimport cv2import numpy as npimg1 = cv2.imread(&#x27;csdn.png&#x27;)img2 = np.rot90(img1)orb = cv2.ORB_create(50)# 特征点检测和提取特征点描述子kp1, des1 = orb.detectAndCompute(img1, None)kp2, des2 = orb.detectAndCompute(img2, None)# 创建 BFMatcher 对象bf = cv2.BFMatcher(cv2.NORM_L2)# 根据描述子匹配特征点.matches = bf.match(des1, des2)# 画出50个匹配点img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches[:50], None, flags=2)cv2.imshow(&quot;ORB&quot;, img3)cv2.waitKey(0)","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"},{"title":"OpenCV环境配置（win10+opencv3.4.1+vs2017）","slug":"2019-01-07-opencv-setup","date":"2019-01-06T16:00:00.000Z","updated":"2022-03-25T13:50:44.667Z","comments":true,"path":"2019/01/07/2019-01-07-opencv-setup/","link":"","permalink":"https://aicc-cn.github.io/2019/01/07/2019-01-07-opencv-setup/","excerpt":"","text":"简介： OpenCV是一个基于BSD许可（开源）发行的跨平台计算机视觉库，可以运行在Linux、Windows、Android和Mac OS操作系统上。它轻量级而且高效——由一系列 C 函数和少量 C++ 类构成，同时提供了Python、Java、Ruby、MATLAB等语言的接口，实现了图像处理和计算机视觉方面的很多通用算法。 系统环境系统：windows 10 - 1809 OpenCV版本：3.4.1 Microsoft Visio Studio：2017 环境安装 默认vs2017已经安装。 下载路径：从OpenCV Library下载安装包并安装 进入官网首页，点击【Releases】 在Releases 界面下找到3.4.1版本, 因为是Windows平台且使用安装包方式，故点击【Win pack】 下载完毕后，【双击安装包】开始安装 我这里是安装在D盘，不用再新建文件夹，解压后会自动创建一个【opencv】的文件夹。 注意：安装路径不要有中文，避免出现问题。 等待，直到安装完毕。 配置OpenCV的系统环境 在【桌面】或者【资源管理器】-&gt; 【右击】【此电脑】-&gt; 选择【属性】 选择【高级系统设置】-&gt; 点击【环境变量】 环境变量界面分为两部分，上方为【用户变量】，下方为【系统变量】 在【系统变量】列表中找到【Path】，双击打开 在打开界面的右侧列表中选择【浏览】,进入opencv安装路径并选取XXXX\\opencv\\build\\x64\\vc14\\bin目录，逐级【确定】保存。 系统环境变量设置完毕。 在VS2017项目中配置OpenCV【新建空项目】 【新建cpp文件】右击-&gt;【源文件】-&gt; 【添加】-&gt; 【新建项】 【属性管理】VS2017上方菜单栏找到【视图】–&gt;【其他窗口】–&gt;【属性管理器】，点击开启 【Debug X64设置】右击【Debug|x64】,点击【属性】 在左侧【通用属性】菜单下找到【VC++目录】，点击，右侧找到【包含目录】，点击，右侧出现【倒三角】，点击，在弹出下拉列表中点击【编辑】 在弹出界面中通过点击右上角【新行】按钮，添加三条路径：【XXXX\\opencv\\build\\include\\opencv2】【XXXX\\opencv\\build\\include\\opencv】【XXXX\\opencv\\build\\include】 在【库目录】添加路径 【XXXX\\opencv\\build\\x64\\vc14\\lib】 在左侧【通用属性】菜单下找到【链接器】，展开菜单，选择【输入】，在右侧找到【附加依赖项】，以【第4步】中同样方式打开【编辑】，在上方空白处手动键入:【opencv_world341d.lib】。 保存退出 此处键入的文件随版本号不同而不同，若非OpenCV 3.4.1版本可自行进入【XXXX\\opencv\\build\\x64\\vc14\\lib】目录查看文件名并键入。 【Release|x64设置】重复执行【Debug|X64设置】–&gt; 不同的就是【依赖项】的选择。 文件夹下会看到几个几乎同名的文件，区别仅仅为文件名末尾有无字母d，其中d代表debug版本，其他为release版本，以3.4.1版本为例，配置【Debug|x64】时使用opencv_world341d.lib，配置【Release|x64】时使用opencv_world341.lib. 【环境搭建完成】功能测试【测试代码】123456789101112#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;int main()&#123; Mat image = imread(&quot;opencv.png&quot;); imshow(&quot;显示图像&quot;, image); waitKey(0); return 0;&#125; 【运行环境】 【运行结果】 【环境搭建成功】配置属性表 我们知道了如何对新建的项目进行属性配置，但若每次都从头进行一遍是十分繁琐的，所以可以通过【配置属性表并导出】，下次新建项目时【导入】，来减少这些不必要的重复劳动。 【新建属性表】 以对【Debug|x64】操作为例，右击，选择【添加新项目属性表】 在跳出的窗口中选择【属性表】格式文件，对文件【命名】,【选择保存位置】，点击【添加】新建成功 此时我们在【Debug|x64】目录下会看到创建好的属性表文件，对其双击打开，以同样的方式配置【包含目录】、【库目录】、【附加依赖项】，保存，对于【Release|x64】同理 相应的位置即可找到这两个文件 【导入属性表】以后再次新建项目只需在属性管理器中通过【添加现有属性表】导入即可完成配置","categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}],"author":"Seven"}],"categories":[{"name":"机器视觉","slug":"机器视觉","permalink":"https://aicc-cn.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://aicc-cn.github.io/tags/OpenCV/"}]}